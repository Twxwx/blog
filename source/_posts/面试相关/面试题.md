---
title: 面试题
date: 2024-07-29 20:51:04
categories:
    - 面试相关
tags:
---

## 分类任务为什么不用均方误差而使用交叉熵？

1. 数据分布的差异。回归任务面对的是连续变量，其数据分布往往呈现出一定的规律性和连续性；而分类任务处理的是离散的类别标签，数据分布通常是散列的，并且每个类别内部可能并没有明显的先后顺序或大小关系。因此，回归任务更关注于数值预测的精度，而分类任务则更侧重于类别判定的准确性和置信度。这种根本的区别导致了损失函数设计的侧重点不同

2. 分类问题会有激活函数这个非线性单元（sigmoid），如果使用 MSE 做损失函数的话计算结果是非凸函数，有多个极值点，不适用做损失函数。并且sigmoid函数和MSE一起使用时会出现梯度消失。

## 欠拟合和过拟合产生的原因和解决方法

### 欠拟合

#### 定义
- 欠拟合是指模型在训练数据和测试数据上都表现较差的现象。欠拟合的模型过于简单，以至于它无法捕捉到数据中的关键特征和模式

#### 原因
1. 模型过于简单，无法捕捉到数据中的关键特征和模式。
2. 选取的特征无法反映数据的真实分布，导致模型性能较差。

#### 解决方法
1. 选择更复杂的模型，增加模型参数的数量，使模型能够捕捉到数据中的关键特征和模式。
2. 通过特征选择、特征提取、特征组合等方法，选取有助于模型预测的特征，提升模型性能。
3. 通过调整学习率、批量大小、迭代次数等超参数，优化模型的训练过程，提升模型性能。

### 过拟合

#### 定义
- 过拟合是指模型在训练数据上表现良好，但在测试数据或新数据上表现较差的现象。过拟合的模型过于复杂，以至于它能够“记住”训练数据中的噪声，而非真实的数据分布。

#### 原因
1. 模型参数过多，过于复杂，容易导致模型过度拟合训练数据中的噪声。
2. 数据量不足以支持复杂数学公式，容易导致模型过拟合。
3. 数据噪声较大，模型可能学习到数据中的噪声，而非真实的数据分布。

#### 解决方法
1. 通过增加训练数据量，可以减小模型对噪声的敏感度，提升模型的泛化能力
2. 选择更简单的模型，减少模型参数的数量，避免模型过度拟合训练数据
3. 通过L1正则化、L2正则化等方法，限制模型参数的大小，防止模型过拟合
4. 在训练过程中，当验证集的损失不再显著下降时，提前停止训练，避免模型过拟合
5. 通过对训练数据进行旋转、翻转、缩放等数据增强操作，增加数据的多样性，提升模型的泛化能力

## 梯度消失和梯度爆炸产生的原因和解决方法

### 梯度消失

#### 定义

- 梯度消失问题主要出现在深层神经网络中，由于权重的累积，梯度在传播过程中会逐渐趋于零，导致模型无法学习到有效的梯度信息

#### 原因
1. 在深层网络中，不同层的学习速度差异很大
2. 采用了不合适的损失函数，比如sigmoid。反向传播时，每一层都要乘以激活函数的导数，sigmoid的导数会导致梯度不断衰减

### 梯度爆炸

#### 定义

- 梯度爆炸问题是指在某些情况下，梯度在传播过程中会逐渐增大，导致梯度计算过大，导致梯度下降算法不稳定或崩溃

#### 原因

1. 在深层网络中，不同层的学习速度差异很大
2. 权值初始化值太大

### 解决方法

1. 梯度剪切主要针对梯度爆炸，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

2. 采用权重正则化（weithts regularization），正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。

3. 选择relu等梯度大部分落在常数上的激活函数

4.  batch normalization 通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题

5. 使用残差网络

## 大模型存在的问题

1. 过度消耗数据和算力。大模型的参数量已达到万亿级别，训练数据规模和算力消耗与参数规模成正比。
2. 灾难性遗忘。在新任务上训练会损害之前任务的性能；在问题求解阶段，无法记住处理过的数据或场景，
3. 黑盒模型逻辑推理能力弱，大模型缺乏“分而治之”能力，在处理需要逻辑、数值推理等复杂问题时表现不佳，无法举一反三、触类旁通。
4. 大模型不知道自己错了，也不知道为啥错，更做不到知错就改。