---
title: 面试汇总
date: 2024-01-17 14:36:23
categories:
    - 面试
tags:
---

- 手撕 multi-head self-attention
- 说出 pre-norm 和 post-norm 的区别
- ChatGLM、LLaMA、Qwen等大模型的区别
- 如何优化 prompt
- 大模型的微调方法
- 当前大模型检索有什么问题？如何进行 RAG 优化？
- 手撕 transformer decoder 部分的inference过程
- 如何解决含有象征、隐喻等修辞手法的文本分类问题
- 如何评估大模型的效果及安全性
- 常用的文本数据增强方法
- KL散度和交叉熵的联系与区别
- attention结构中Q、K、V的含义及作用
- 绝对位置和相对位置的区别（举例回答）
- layer normalization 中两个可学习参数的作用
- 中文场景的nlu任务和英文场景有什么不同
- 模型蒸馏的具体实现过程
- 模型量化
- 模型分布式部署
- 手撕 softmax和交叉熵
- softmax函数在梯度反传过程中会出现梯度的钝化，该怎么解决？
- 在文本分类任务中，如果打乱文本中字或词的顺序，是否会对分类结果产生影响
- 模型小型化技术有哪些？
- 使用大模型的落地场景
- 如何在一个3GB内存空间中部署一个深度学习模型
- 说一下对多任务训练和多领域训练的理解
- 数据并行和模型并行的区别
- 混合精度训练的原理，有哪些优缺点，针对这些优缺点如何改进
