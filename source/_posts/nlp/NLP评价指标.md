---
title: NLP评价指标
date: 2024-04-04 14:58:41
categories:
    - NLP
tags:
---

## BLEU(Bilingual Evaluation Understudy)

- 定义：BLEU是一种用于评估机器翻译质量的指标，它通过比较机器翻译输出和一组参考翻译之间的n-gram重叠来评分。

- 计算方式：BLEU分数是通过计算机器翻译输出和参考翻译之间的n-gram精确匹配度，并通过短语长度惩罚因子来调整得到的。BLEU分数的范围从0到1，其中1表示完美的匹配。

- 应用场景：BLEU主要用于机器翻译任务，但也可用于其他文本生成任务，如文本摘要。

![](/img/note/202408261717.png)

![](/img/note/202408261718.png)

![](/img/note/202408261719.png)

## Rouge(Recall-Oriented Understudy for Gisting Evaluation)

- 定义：ROUGE是一组用于评估自动文摘和机器翻译质量的指标，它主要关注召回率，但也考虑精确率。

- 计算方式：ROUGE包括多种指标，如ROUGE-N（基于n-gram的重叠）、ROUGE-L（基于最长公共子序列）和ROUGE-W（加权最长公共子序列）。每种指标都有其特定的计算方式，主要关注生成的文本和参考文本之间的重叠程度。

- 应用场景：ROUGE广泛用于自动文摘和机器翻译任务，特别是在需要考虑文本的整体相似性时。

![](/img/note/202408261720.png)

![](/img/note/202408261721.png)

## perplexity（困惑度、复杂度）

![](/img/note/202408261722.png)

## 二分类任务评价指标

### 混淆矩阵

![](/img/note/202409181503.png)

### 准确率

![](/img/note/202409181504.png)

### 精确率

![](/img/note/202409181505.png)

### 召回率

![](/img/note/202409181506.png)

### 精确率和召回率的区别

1. 精确率高意味着较少的误报，即较少的非相关实例被错误地识别为相关。例如在推荐系统中，如果系统频繁推荐用户不喜欢的产品，将导致用户体验下降。
2. 召回率高意味着较少的漏报，即更多的相关实例被正确地识别出来。例如在疾病筛查中，错过一个病例的代价可能是生命，因此需要更高的召回率。

### F1分数

![](/img/note/202409181507.png)

### ROC和AUC

#### 概念

- ROC 是不同分类阀值下的正类率（TPR）和假正类率（FPR）构成的曲线。对某个分类器而言，我们可以根据其在测试样本上的表现得到一个TPR和FPR点对。这样，此分类器就可以映射成ROC平面上的一个点。调整这个分类器分类时候使用的阈值，我们就可以得到一个经过(0, 0)，(1, 1)的曲线，这就是此分类器的ROC曲线。

- Area Under roc Curve(AUC) 的值就是处于 ROC 曲线下方的面积大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能，是一种用来度量分类模型好坏的一个标准。物理意义是：随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本的概率的概率。

#### 计算方式

- 给定正样本Ｍ个，负样本Ｎ个，以及他们的预测概率，穷举所有的正负样本对，如果正样本的预测概率大于负样本的预测概率，那么就+1；如果如果正样本的预测概率等于负样本的预测概率，那么就＋0.5,　如果正样本的预测概率小于负样本的预测概率，那么就+0；最后把统计处理的个数除以Ｍ×Ｎ就得到AUC。

```python
def AUC(label,pre):
    pos = []  # 正样本index
    neg = []  # 负样本index
    auc = 0
    for i in range(len(label)):
        if(label[i] == 1):
            pos.append(i)
        else:
            neg.append(i)
    for i in pos:
        for j in neg:
            if(pre[i] > pre[j]):
                auc += 1
            if(pre[i] == pre[j]):
                auc += 0.5
            else:
                auc += 0
    return auc / (len(pos) * len(neg))

if __name__ == "__main__":
    # 1为正样本，0为负样本
    label = [1, 0, 0, 0, 1, 0, 1, 0] 
    pre = [0.9, 0.8, 0.3, 0.1, 0.4, 0.9, 0.66, 0.7]
    print(AUC(label, pre))
    from sklearn import metrics
    auc = metrics.roc_auc_score(label, pre)
    print('sklearn', auc)
```

### F1和AUC的区别

- AUC 希望训练一个尽量不误报的模型，也就是知识外推的时候倾向保守估计，而F1希望训练一个不放过任何可能的模型，即知识外推的时候倾向激进。

## 多分类任务评价指标

- 考虑类别不平衡的情况，即不同类别的样本数量可能不同

### 宏平均

![](/img/note/202409181508.png)