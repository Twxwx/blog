---
title: 文本向量嵌入
date: 2024-02-03 16:01:19
categories:
    - NLP
tags:
---

## 何为Embedding

- Embedding 是将高维数据映射到低维空间的方法，通常用于将离散的、非连续的数据（文字、图片、音频）转换为连续的向量表示，以便于计算机进行处理
- 简单来说，Embedding 帮助计算机来理解如人类信息所代表的“含义”，Embedding 可以用来获取文本、图像、视频、或其他信息的特征“相关性”，这种相关性在应用层面常用于搜索、推荐、分类、聚类。

![](/img/note/202402041352.png)

## 文本特征技术

### 2013年前

#### OneHot

``` bash
我 1 0 0 0
们 0 1 0 0
相 0 0 1 0
信 0 0 0 1
.
.
.
```

- 在实际使用时，往往不会这么简单的用 0、1 来表示，因为每个字在句子中的作用是不一样的，所以一般会给不同的Token赋予不同的权重。

#### TF-IDF（Term Frequency，Inverse Document Frequency）

- TF 就是词频，IDF 具体等于文档总数/包含该词的文档数。比如「的」在一句话（或一段文档）中概率很高，但几乎所有句子（或文档）都有「的」，IDF 接近 1；相反如果一个词在句子中概率高，但包含该词的文档比较少，IDF 就比较大，最后结果也大。词在单个文档或句子中是高概率的，但在所有文档或句子中是低概率的，可以给这个词赋予较高权重。

#### 缺点

- 数据维度太高：太高的维度会导致向量在空间中聚集在一个非常狭窄的角落，模型难以训练。
- 数据稀疏，向量之间缺乏语义上的交互（语义鸿沟）。


### 2013年后

#### 核心思想

- 把特征固定在某一个维度，避免了维度过高的问题。
- 利用自然语言文本的上下文关系学习一个稠密表示。

#### word2vec

[论文链接](https://arxiv.org/pdf/1301.3781.pdf)

- 提出两个模型CBOW (Continuous Bag-of-Words Model) 和 Skip-gram (Continuous Skip-gram Model) 都是只有三层，即输入层、映射层和输出层。CBOW 模型用上下文的词向量作为输入，映射层在所有的词间共享，输出层为一个分类器，目标是使当前词的概率最大。Skip-gram模型与CBOW的输入跟输出恰好相反，输入层为当前词向量，输出层是使得上下文的预测概率最大，如下图所示。训练采用SGD。

![](/img/note/202402041122.png)


#### nn.Embedding

- 在PyTorch中，针对词向量有一个专门的层nn.Embedding，用来实现词与词向量的映射。Embedding的权重是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。

- 简单概括就是一个简单的存储固定大小的词典的嵌入向量的查找表，意思就是说，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。






