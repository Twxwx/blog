---
title: NLP技术发展历程
date: 2024-09-21 11:28:02
categories:
    - NLP
tags:
---

## 背景

- 自然语言处理（Natural Language Processing）简单来说即是计算机接受用户自然语言形式的输入，并在内部通过人类所定义的算法进行加工、计算等系列操作，以模拟人类对自然语言的理解，并返回用户所期望的结果。

## NLP领域
- 可以分为自然语言理解（NLU）和自然语言生成（NLG）两种。NLU侧重于如何理解文本，包括文本分类、命名实体识别、指代消歧、句法分析、机器阅读理解等；NLG则侧重于理解文本后如何生成自然文本，包括自动摘要、机器翻译、问答系统、对话机器人等。两者间不存在有明显的界限，如机器阅读理解实际属于问答系统的一个子领域。

1. 【文本检索】：多用于大规模数据的检索，典型的应用有搜索引擎
2. 【机器翻译】：跨语种翻译，该领域目前已较为成熟。目前谷歌翻译已用上机翻技术
3. 【文本分类/情感分析】：本质上就是个分类问题。目前也较为成熟，难点在于多标签分类（即一个文本对应多个标签，把这些标签全部找到）以及细粒度分类（二级情感分类精度很高，即好中差三类，而五级情感分类精度仍然较低，即好、较好、中、较差、差）
4. 【信息抽取】：从不规则文本中抽取想要的信息，包括命名实体识别、关系抽取、事件抽取等。应用极广。
5. 【序列标注】：给文本中的每一个字/词打上相应的标签。是大多数NLP底层技术的核心，如分词、词性标注、关键词抽取、命名实体识别、语义角色标注等等。曾是HMM、CRF的天下，近年来逐步稳定为BiLSTM-CRF体系。
6. 【文本摘要】：从给定的文本中，聚焦到最核心的部分，自动生成摘要。
7. 【问答系统】：接受用户以自然语言表达的问题，并返回以自然语言表达的回答。常见形式为检索式、抽取式和生成式三种。近年来交互式也逐渐受到关注。典型应用有智能客服
8. 【对话系统】：与问答系统有许多相通之处，区别在于问答系统旨在直接给出精准回答，回答是否口语化不在主要考虑范围内；而对话系统旨在以口语化的自然语言对话的方式解决用户问题。对话系统目前分闲聊式和任务导向型。
9. 【知识图谱】：从规则或不规则的文本中提取结构化的信息，并以可视化的形式将实体间以何种方式联系表现出来。图谱本身不具有应用意义，建立在图谱基础上的知识检索、知识推理、知识发现才是知识图谱的研究方向
10. 【文本聚类】：从大规模文本数据中自动发现规律。核心在于如何表示文本以及如何度量文本之间的距离

## 发展历程

### NLP规则时代

- 模拟人类学习语言的习惯，以语法规则为主流。除了参照乔姆斯基文法规则定义的上下文无关文法规则外，NLP领域几乎毫无建树。

### NLP统计时代

- 70年代开始统计学派盛行，NLP转向统计方法，此时的核心是以具有马尔科夫性质的模型（包括语言模型，隐马尔可夫模型等）。
- 2001年，神经语言模型，将神经网络和语言模型相结合，应该是历史上第一次用神经网络得到词嵌入矩阵，是后来所有神经网络词嵌入技术的实践基础。也证明了神经网络建模语言模型的可能性。
- 2001年，条件随机场CRF，从提出开始就一直是序列标注问题的利器，即便是深度学习的现在也常加在神经网络的上面，用以修正输出序列。
- 2003年，LDA模型提出，概率图模型大放异彩，NLP从此进入“主题”时代。Topic模型变种极多，参数模型LDA，非参数模型HDP，有监督的LabelLDA，PLDA等。
- 2008年，分布式假设理论提出，为词嵌入技术的理论基础。在统计时代，NLP专注于数据本身的分布，如何从文本的分布中设计更多更好的特征模式是这时期的主流。在这期间，还有其他许多经典的NLP传统算法诞生，包括tfidf、BM25、PageRank、LSI、向量空间与余弦距离等。值得一提的是，在20世纪80、90年代，卷积神经网络、循环神经网络等就已经被提出，但受限于计算能力，NLP的神经网络方向不适于部署训练，多停留于理论阶段。

### NLP深度时代

- 2013年，word2vec提出，NLP的里程碑式技术，从此万物embedding2013年，CNNs/RNNs/Recursive NN，随着算力的发展，神经网络可以越做越深，之前受限的神经网络不再停留在理论阶段。在图像领域证明过实力后，Text CNN问世；同时，RNNs也开始崛起。在如今的NLP技术上，一般都能看见CNN/LSTM的影子。本世纪算力的提升，使神经网络的计算不再受限。有了深度神经网络，加上嵌入技术，人们发现虽然神经网络是个黑盒子，但能省去好多设计特征的精力。至此，NLP深度学习时代开启。
- 2014年，seq2seq提出，在机器翻译领域，神经网络碾压基于统计的SMT模型。
- 2015年，attention提出，可以说是NLP另一里程碑式的存在。带attention的seq2seq，碾压上一年的原始seq2seq。
- 2017年末，Transformer提出。
- 2018年末，BERT提出，横扫11项NLP任务，奠定了预训练模型方法的地位，NLP又一里程碑诞生。光就SQuAD2.0上前6名都用了BERT技术就知道BERT的可怕。深度学习时代，神经网络能够自动从数据中挖掘特征，人们从复杂的特征中脱离出来，得以更专注于模型算法本身的创新以及理论的突破。并且深度学习从一开始的机器翻译领域逐渐扩散到NLP其他领域，传统的经典算法地位大不如前。但神经网络似乎一直是个黑箱，可解释性一直是个痛点，且由于其复杂度更高，在工业界经典算法似乎还是占据主流。

## NLP的目前难点

- 中文分词：同一个任务，同一个模型在英文语料的表现上一般要比中文语料好。无论是基于统计的还是基于深度学习的NLP方法，分词都是第一步。分词表现不好的话，后面的模型最多也只能尽力纠偏
- 词义消歧：很多单词不只有一个意思。另一个较难的是指代消歧，即句子中的指代词还原，如“小明受到了老师的表扬，他很高兴”，这个“他”是指“小明”还是指“老师”。
- 二义性：有些句子，往往有多种理解方式，其中以两种理解方式的最为常见，称二义性。如“我们两人一组”，究竟是“我们两人/一组”（这个组就2个人），还是“我们/两人一组”（每组2个人）
- OOV问题：随着词嵌入技术大热后，使用预训练的词向量似乎成为了一个主流。但有个问题就是，数据中的词很可能不在预训练好的词表里面，此即OOV（out of vocabulary）。目前主流方法是要么当做UNK处理，要么生成随机向量或零向量处理，都存在一定的弊端。
- 文本相似度计算：文本相似度计算依旧算是难点之一。目前主流认可的是用余弦相似度，除了余弦相似度外，还有欧式距离、曼哈顿距离、向量內积。目前还处于经验阶段，缺少真正的理论证明。
- 文本生成的评价指标：目前文本生成的评价指标多用BLEU或者ROUGE，但这两个指标都是基于n-gram的，也就是说会判断生成的句子与标签句子词粒度上的相似度。然而由于自然语言的特性（同一个意思可以有多种不同的表达），会出现生成的句子尽管被人为判定有意义，在BLEU或ROUGE上仍可能会得到很低的分数的情况。
