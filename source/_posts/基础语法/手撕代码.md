---
title: 手撕代码
date: 2024-04-12 19:58:08
categories:
    - 手撕代码
tags:
---

## 手撕注意力机制

```python
import torch
import torch.nn as nn
import math
import torch.nn.functional as F

class MultiheadAttention(nn.Module):
    def __init__(self, d_model: int, heads: int = 8):
        super().__init__()
        self.d_model = d_model
        self.heads = heads
        self.d_k = self.d_model // self.heads
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.o_proj = nn.Linear(d_model, d_model)
    
    def forward(
            self,
            q: torch.Tensor,
            k: torch.Tensor,
            v: torch.Tensor,
            mask: torch.Tensor = None
        ):
        bsz = q.shape[0]
        # [bsz, seq_len, d_ff] -> [bsz, seq_len, d_model] -> [bsz, seq_len, heads, d_k] -> [bsz, heads, seq_len, d_k]
        q = self.q_proj(q).view(bsz, -1, self.heads, self.d_k).transpose(1, 2)
        k = self.q_proj(k).view(bsz, -1, self.heads, self.d_k).transpose(1, 2)
        v = self.q_proj(v).view(bsz, -1, self.heads, self.d_k).transpose(1, 2)
        # 计算注意力分数 scores: [bsz, heads, seq_len, seq_len] 
        attn = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)
        # 生成 casual mask
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        scores = F.softmax(attn, dim = -1)
        # [bsz, heads, seq_len, d_k]
        out = torch.matmul(scores, v)
        concat = out.transpose(1, 2).contiguous().view(bsz, -1, self.d_model)
        output = self.o_proj(concat)
        return output

if __name__ == "__main__":
    bsz = 4
    seq_len = 1024
    d_ff = 512
    x = torch.randn(bsz, seq_len, d_ff)
    attention = MultiheadAttention(d_model=512, heads=8)
    mask = torch.tril(torch.ones(seq_len, seq_len))
    out = attention(x, x, x, mask)
    print(out.shape)
```

## 手撕 softmax

```python
def softmax(x: torch.Tensor):
    exp_x = torch.exp(x - x.max(dim=-1, keepdim=True).values)
    return exp_x / exp_x.sum(dim=-1, keepdim=True)
```

## 手撕 cross_entropy

```python 
def cross_entropy(x: torch.Tensor, y: torch.Tensor):
    return -torch.sum(y * torch.log(x), dim=1).mean()
```
