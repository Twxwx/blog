---
title: 大数据相关
date: 2024-01-11 18:24:52
categories:
    - 大数据相关
tags:
---

## hadoop
1. hadoop简介
- Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 Hadoop实现了一个分布式文件系统HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的硬件上；而且它提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。

2. hadoop优点
Hadoop 以一种可靠、高效、可伸缩的方式进行数据处理。
- 可靠性: Hadoop将数据存储在多个备份，Hadoop提供高吞吐量来访问应用程序的数据。
- 高扩展性： Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。
- 高效性： Hadoop以并行的方式工作，通过并行处理加快处理速度。
- 高容错性： Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
- 低成本： Hadoop能够部署在低廉的（low-cost）硬件上。

## Hive
1. Hive 简介
- Hive 是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Hive的优点是学习成本低，可以通过类似SQL语句实现快速MapReduce统计，使MapReduce变得更加简单，而不必开发专门的MapReduce应用程序。hive十分适合对数据仓库进行统计分析。

2. Hive特点：
- 简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；
- 灵活性高，可以自定义用户函数 (UDF) 和存储格式；
- 为超大的数据集设计的计算和存储能力，集群扩展容易;
- 统一的元数据管理，可与 presto／impala／sparksql 等共享数据；
- 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。

## Spark
1. Spark简介
- Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark拥有Hadoop MapReduce所具有的优点，Spark在Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark性能以及运算速度高于MapReduce。

2. Spark优点
- 计算速度快: 因为spark从磁盘中读取数据，把中间数据放到内存中，完成所有必须的分析处理，将结果写回集群，所以spark更快。
- Spark 提供了大量的库: 包括Spark Core、Spark SQL、Spark Streaming、MLlib、GraphX。
- 支持多种资源管理器: Spark 支持 Hadoop YARN，及其自带的独立集群管理器
- 操作简单: 高级 API 剥离了对集群本身的关注，Spark 应用开发者可以专注于应用所要做的计算本身

## pyspark
1. pyspark简介
- pyspark 是一个基于 Python 的 Spark 编程接口，可以用于大规模数据处理、机器学习和图形处理等各种场景。

2. 基本概念
- RDD（Resilient Distributed Datasets）是pyspark的核心概念，是一种弹性分布式数据集。它是Spark中的基本数据结构，可以看做是一个分布式的未被修改的数据集合。RDD可以被分区和并行处理，支持容错和自动恢复，保证了数据的高可靠性和高可用性。
- DataFrame是一种类似于关系型数据库中的表格的数据结构。它提供了一种高级的抽象层次，可以将数据组织成一组命名的列。DataFrame支持类似于SQL的查询，可以很方便地进行数据筛选、过滤、排序和统计等操作。
- SparkContext是pyspark中的一个核心概念，是Spark应用程序的入口。它负责连接Spark集群，并与集群中的其他节点进行通信。SparkContext提供了许多Spark操作的入口点，如创建RDD、累加器和广播变量等。

## spark与hadoop的不同点
1. 应用场景不同
- Hadoop是一个分布式数据存储架构，它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，降低了硬件的成本。
- Spark是那么一个专门用来对那些分布式存储的大数据进行处理的工具，它要借助hdfs的数据存储。
2. 处理速度不同
- hadoop的MapReduce是分步对数据进行处理的，从磁盘中读取数据，进行一次处理，将结果写到磁盘，然后在从磁盘中读取更新后的数据，再次进行的处理，最后再将结果存入磁盘，这存取磁盘的过程会影响处理速度。
- spark从磁盘中读取数据，把中间数据放到内存中，完成所有必须的分析处理，将结果写回集群，所以spark更快。
3. 容错性不同
- Hadoop将每次处理后的数据都写入到磁盘上，基本谈不上断电或者出错数据丢失的情况。
- Spark的数据对象存储在弹性分布式数据集 RDD，RDD是分布在一组节点中的只读对象集合，如果数据集一部分丢失，则可以根据于数据衍生过程对它们进行重建。而且RDD 计算时可以通过 CheckPoint 来实现容错。

## spark与hadoop的联系
Hadoop 提供分布式数据存储功能 HDFS，还提供了用于数据处理的 MapReduce。 MapReduce 是可以不依靠 spark 数据的处理的。当然 spark 也可以不依靠 HDFS 进行运作，它可以依靠其它的分布式文件系统。但是两者完全可以结合在一起，hadoop 提供分布式集群和分布式文件系统，spark 可以依附在 hadoop 的 HDFS 代替 MapReduce 弥补 MapReduce 计算能力不足的问题。

