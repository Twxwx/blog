---
title: Mamba Linear-Time Sequence Modeling with Selective State Spaces
date: 2024-01-19 22:26:01
categories:
    - 论文阅读
tags:
---

[论文链接]()

## 存在问题
- Transformer在处理长序列时存在计算效率问题。
- 人们开发了例如线性注意力，门控卷积和循环模型，以及结构化状态空间模型(SSMs)。然而，这些模型在处理离散和信息密集型数据（如文本）时表现不佳。

## 结构化状态空间序列模型 (Structured State Space Sequence Models)
![](/img/paper/202401231514.png)
![](/img/paper/202401231505.png)
- 递归神经网络（RNN）和卷积神经网络（CNN）的组合。具体而言，将连续参数离散化，然后转变为递归和卷积相结合。 
- 此类模型可以非常有效地计算为递归或卷积，并且序列长度具有线性或近线性缩放。 
- 此外，它们还拥有在某些数据模式中对远程依赖关系进行建模的原则性机制。
- 在音频和视觉等涉及连续信号数据的领域取得了成功。然而，它们在对离散和信息密集的数据（例如文本）进行建模方面效率较低。

## 方法
- 状态选择机制：简单地让 SSM 参数作为输入的函数，可以解决其离散模态的弱点，允许模型根据当前标记选择性地沿序列长度维度传播或忘记信息。
- 硬件感知算法：设计了一种硬件感知的并行算法，以循环模式计算模型，避免了在GPU内存层次结构中不必要的状态扩展，无需注意力机制，甚至不需要 MLP 模块。

## 模型架构
- 选择性状态空间模型
![](/img/paper/202401211801.png)
![](/img/paper/202401211804.png)
- 硬件感知的状态扩展
![](/img/paper/202401211805.png)
![](/img/paper/202401211722.png)
- 简化的SSM架构
![](/img/paper/202401211808.png)

## 贡献
- 快速推理（吞吐量比 Transformer 高 5 倍）
- 序列长度线性缩放，并且其性能在高达百万长度序列的实际数据上得到提高。 
- 在语言、音频和基因组学等多种模式上实现了最先进的性能。

