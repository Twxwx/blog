---
title: 多模态
date: 2024-01-17 14:37:04
categories:
    - 多模态
tags:
---

## 概述

![](/img/note/202403081417.png)

![](/img/note/202403081418.png)

## CLIP

![](/img/note/202403081419.png)

## ViLT

![](/img/note/202403081420.png)

![](/img/note/202403081421.png)

## VLMO

![](/img/note/202403081422.png)

![](/img/note/202403081423.png)

![](/img/note/202403081424.png)

## BLIP

![](/img/note/202403081425.png)

![](/img/note/202403081426.png)

## BLIP-2

![](/img/note/202403081427.png)

![](/img/note/202403081428.png)

## CogVLM-17B

- CogVLM 核心的思想是“视觉优先”，使用5B参数的视觉编码器和6B参数的视觉专家模块，总共11B参数建模图像特征，甚至多于文本的7B参数量。之前的多模态模型通常都是将图像特征直接对齐到文本特征的输入空间去，且图像特征的编码器通常较小，图像可以看成是文本的“附庸”，效果有限。

- 先前的技术路线通常采用一种“浅对齐”(shallow alignment)方式。而 CogVLM 往大语言模型的每个 decoder 层添加视觉专家模块，实现了更深层次的对齐

![](/img/note/202403221719.png)

### 预训练
- 阶段一：利用 image captioning loss 训练模型自回归生成图像 caption
- 阶段二：利用 Referring Expression Comprehension（根据图片的文本描述得到物体的边缘框坐标信息）。prompt 是“图像中某个物体的位置是什么”；label为“[x0,y0,x1,y1]"。

## LLaVA

![](/img/note/202403081429.png)