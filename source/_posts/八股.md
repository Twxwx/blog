---
title: 八股
date: 2024-03-20 15:35:40
categories:
    - 面试相关
tags:
---

## 什么是R-dropout？
## 分类任务里，交叉熵损失函数，某一类数据缺失，在损失里有什么处理办法？
## 什么时候用bert，什么时候用 decoderd-only？
## 梯度消失和梯度爆炸，一般出现在什么地方

### 原因

#### 梯度消失

- 当梯度消失时，梯度值在反向传播过程中逐渐变小，导致底层网络参数几乎没有更新，难以学习到有效的表示。这种情况通常发生在网络的深层结构中，尤其是在使用sigmoid或tanh等激活函数时，因为它们的导数在接近饱和区域时接近于0。当梯度经过多个层级的乘法运算后，梯度值会指数级地衰减，从而导致梯度消失问题。梯度消失对神经网络的训练造成以下几个影响：

    - 难以学习长期依赖性：深层网络在处理长期依赖性任务时会受到影响。由于梯度逐渐消失，网络无法有效地捕捉和传递远距离的依赖关系，导致难以学习到长期记忆和时间序列中的重要模式。

    - 参数更新缓慢：由于梯度变小，网络参数的更新速度变得非常缓慢。网络需要更多的训练迭代才能收敛到较好的解，导致训练时间变长。

    - 局部最优解：当梯度消失时，网络可能会陷入局部最优解或鞍点，并且很难从中跳出。梯度消失限制了网络在参数空间中的搜索能力，可能导致网络收敛到次优解。

#### 梯度爆炸

- 当梯度爆炸时，梯度值在反向传播过程中逐渐增大，导致底层网络参数更新过大，造成不稳定的训练。当梯度经过多个层级的乘法运算后，梯度值会指数级地增加，从而导致梯度爆炸问题。梯度爆炸的问题会导致训练过程变得不稳定，可能导致网络无法收敛或收敛速度非常缓慢。梯度爆炸会对神经网络的训练产生以下影响：

    - 不稳定的更新：梯度爆炸使得参数的更新量非常大，可能会导致模型在每次更新时产生剧烈的波动，使网络参数无法稳定地收敛到最优解。

    - 数值溢出：当梯度值非常大时，参数更新可能会导致数值溢出的情况发生。这会导致参数的值变得异常大或异常小，使得网络无法正常运行。

    - 训练不收敛：梯度爆炸可能会导致训练过程不收敛或收敛速度非常慢。由于参数更新过大，网络可能无法找到合适的参数组合来最小化损失函数，导致训练过程无法达到预期的性能。

    - 参数不可用：梯度爆炸可能会导致某些参数的值变得非常大，超过了数值范围的上限，使得这些参数变得不可用。这会导致网络无法进行正常的前向传播和反向传播。

### 解决方法

- 权重初始化：使用合适的权重初始化方法，如Xavier初始化或He初始化，可以减轻梯度消失和梯度爆炸问题。

- 激活函数选择：使用合适的激活函数，如ReLU、LeakyReLU等，可以缓解梯度消失问题。另外，使用激活函数前的归一化方法，如批归一化（Batch Normalization），也可以帮助减轻梯度消失和梯度爆炸问题。

- 梯度裁剪：通过对梯度进行裁剪，限制其最大值，可以防止梯度爆炸问题。

- 使用残差连接（Residual Connection）：在深层网络中使用残差连接可以有效减轻梯度消失问题，使梯度能够更好地传递到底层网络。

## 讲讲NMS

### 概述

- 非极大值抑制（Non-maximum supression）简称NMS，其作用是去除冗余的检测框，核心思想是搜索目标局部范围内的边界框置信度最大的这个最优值，去除目标邻域内的冗余边界框。

![](/img/note/202403202340.png)

### 具体步骤

1. 先将所有的边界框按照类别进行区分
2. 把每个类别中的边界框，按照置信度从高到低进行降序排列
3. 选择某类别所有边界框中置信度最高的边缘框
4. 将该类别其他边缘框与最高置信度边缘框计算IOU
5. 将IOU与预设阈值进行比较，若某个边缘框与最高置信度边缘框的IOU大于阀值，即视为冗余边缘框，并且移除该边缘框；
6. 重复这个流程便可去除冗余框得到最准确的结果

## 谈谈对于大模型的理解，怎么算是大模型
## mAP怎么计算的

- TP（True Positives）意思是“被分为正样本，并且分对了”，
- TN（True Negatives）意思是“被分为负样本，而且分对了”，
- FP（False Positives）意思是“被分为正样本，但是分错了”，
- FN（False Negatives）意思是“被分为负样本，但是分错了”

- 计算 Precision 和 Recall 

![](/img/note/202403210906.png)

- 绘制PR曲线并计算AP，对所有类别取平均即可计算得到mAP

![](/img/note/202403210907.png)

## BN层训练和推理有什么区别？

- 在训练时，是对每一批的训练数据进行归一化。使用BN的目的就是每个批次分布稳定。当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

- 而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，也就是使用全局统计量来代替批次统计量。