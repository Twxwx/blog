---
title: 对比学习
date: 2024-01-30 18:33:45
categories:
    - 对比学习
tags:
---

## 对比学习:

- 对比学习是一种基于对比思想的判别式表示学习方法，主要用来做无监督、自监督的表示学习，是一种利用无标签数据进行表示学习的思想。

- 对比学习采用的具体思想：将样例与与它语义相似的例子（正样例）和与它语义不相似的例子（负样例）进行对比，希望通过设计模型结构和对比损失，使语义相近的例子对应的表示在表示空间更接近，语义不相近的例子对应的表示距离更远，以达到类似聚类的效果。

## 对比损失

- 在有了正负例之后，我们需要给模型信号，激励他们拉近正样例间的距离，拉远负样例间的距离，这就要通过设计对比损失来完成

### 原始对比损失

![](/img/note/202402282152.png)

### 三元组损失

![](/img/note/202402282153.png)

### InfoNCE损失

![](/img/note/202402282154.png)

## temperature超参数

- 如果温度系数设的越大，logits分布变得越平滑，那么对比损失会对所有的负样本一视同仁，导致模型学习没有轻重。如果温度系数设的过小，则模型会越关注特别困难的负样本，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。温度系数的作用就是它控制了模型对负样本的区分度。

## SimCLR

### 贡献

- 是一个简单的使用对比学习进行表征学习的框架
- 无监督的对比学习比监督学习更能从强大的数据增强中受益。
- 在表示与对比损失之间引入可学习的非线性变换大大提高了学习到的表示的质量。
- 对比学习受益于更大的批次大小和更长的训练时间。与监督学习一样，对比学习也受益于更深更宽的网络。

### 方法

![](/img/note/202401301837.png)

- 随机数据增强模块，可以随机转换给定的数据样本，生成同一个样本的两个相关的视图，将其视为一个正样本对。
- 应用三种简单的增强方法：随机裁剪然后将大小调整回原始大小，随机颜色失真，以及随机高斯模糊。随机裁剪和颜色失真的组合对于实现良好的性能是至关重要的。
- 较小的网络投影头（projection head） 将表征向量映射到对比学习的隐空间。

![](/img/note/202401301838.jpg)

- 随机抽一个mini-batch，假设有N个样本，每个样本都会产生2个视图，可以得到 2N 个视图。当给定一个正对后，那么就能和 2(N-1) 个样本形成负对。

## SimCSE（Simple Contrastive Sentence Embedding Framework）

- 提出了称为 SimCSE 的对比学习框架，用于学习**句向量表征**。分为无监督SimCSE 和有监督 SimCSE。

![](/img/note/202402012000.png)

### 无监督CSE

![](/img/note/202402012009.png)

### 有监督CSE

![](/img/note/202402012010.png)