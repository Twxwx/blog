---
title: 归一化技术
date: 2024-01-30 15:47:51
categories:
    - 深度学习
tags:
---

## 概述
- 在神经网络学习过程中，归一化的目的是为了使模型收敛，得到学习数据的特性。若在这个过程中，我们没有做归一化处理，那么每层网络输入数据分布在不断变化，很难去学习到特征，模型也就比较难收敛。

- 归一化的主要作用有两个，防止**梯度爆炸**和**梯度消失**。

![](/img/note/202401301559.png)

## BatchNorm

- 将不同样本相同维度的特征处理为相同的分布。BatchNorm针对同一特征，以跨样本的方式开展归一化，因此不会破坏不同样本同一特征之间的关系，但是，特征与特征之间的不再具有可比较性。
- batch 方向做归一化，对小 batchsize 效果不好
- 主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布

## LayerNorm

- 与 Batch normalization 不同，Layer normalization 是在特征维度上进行标准化的，而不是在数据批次维度上。
- LayerNorm的归一化方式：计算一个句子的均值和标准差，然后对句中的每个词做归一化操作。LayerNorm所做的操作，类似于在一个句子中找到一个“语义中心”，然后将句子中的所有词都聚集在这个中心的周围，而句中词与词之间的比较关系不会遭到破坏。
- LayerNorm认为每个样本内的特征具有相同分布，因此针对每一个样本进行归一化处理，保持相同样本内部不同对象的可比较性。  

![](/img/note/202408212326.png)

### PreNorm && PostNorm

![](/img/note/202401301633.png)

- PostNorm 在残差之后做归一化。效果好但是收敛速度慢、参数难调。
- PreNorm 先归一化再残差。可以防止模型的梯度爆炸或者梯度消失，训练更加稳定，但是同等深度效果一般。
- PreNorm结构会过度倾向于恒等分支，从而使得PreNorm倾向于退化为一个“浅而宽”的模型，最终不如同一深度的PostNorm。如果层数少 PostNorm 的效果其实要好一些，如果要把层数加大，为了保证模型的训练，PreNorm 显然更好一些。

## RMSNorm

- RMSNorm是对LayerNorm的一个改进，没有做 re-center 操作（移除了其中的均值项）。
- 它不是使用整个样本的均值和方差，而是使用平方根的均值来归一化，这样做可以降低噪声的影响。

![](/img/note/202408212327.png)

## Instance Normalization

- 用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，IN可以保持每个图像实例之间的独立。

## Group Normalization

- 将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束

## 总结

- CV领域，BN更加合适。如果你的特征依赖于不同样本间的统计参数，那BatchNorm更有效。因为它抹杀了不同特征之间的大小关系，但是保留了不同样本间的大小关系。

- NLP领域，LN更加合适。因为它抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系。对于NLP或者序列任务来说，一条样本的不同特征，其实就是时序上字符取值的变化，样本内的特征关系是非常紧密的。

