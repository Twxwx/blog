---
title: 多任务学习
date: 2024-02-02 17:08:11
categories:
    - 深度学习
tags:
---

## 什么是多任务学习(MTL)？

1. 多任务学习：
    - 给定 m 个学习任务，这 m 个任务或它们的一个子集彼此相关但不完全相同。通过使用所有 m 个任务中包含的知识，有助于改善特定模型的学习。

2. 多任务学习的特点：
    - 具有相关联任务效果相互提升作用，即同时学习多个任务，若某个任务中包含对另一个任务有用的信息，则能够提高在后者上的表现；
    - 具有正则化的效果，即模型不仅需要在一个任务上表现较好，还需要再别的任务上表现好，倾向于学习到在多个任务上表现都比较好的特征；
    - 多任务模型可以共享部分结构，降低内存占用，在推理时减少重复计算，提高推理速度。

3. MTL 处理的任务应具有一定的关联性，若同时学习两个不相关甚至冲突的任务，模型表现可能会受到损害出现经常所说的跷跷板现象，即两个任务联合学习的时候，可能一个任务效果变好，另一个任务效果变差，这个现象称为负迁移。究其本质主要是训练过程中可能出现以下 3 个问题导致的：
    - 多任务梯度方向不一致：同一组参数，不同的任务更新方向不同，导致模型参数出现震荡，任务之间出现负迁移的现象，一般出现在多个任务之间差异较大的场景。
    - 多任务收敛速度不一致：不同的任务收敛速度不一样，有的任务简单收敛速度快，有的任务困难收敛速度慢，导致模型训练一定轮数后，有的任务已经过拟合，有的任务还是欠拟合的状态；
    - 多任务 loss 取值量级差异大：不同的任务 loss 取值范围差异大，模型被 loss 比较大的任务主导，这种情况在两个任务使用不同损失函数，或者拟合值的取值差异大等情况下最为常见。

## 业界方案

### 样本Loss加权

![](/img/note/202409282256.png)

### Shared-Bottom Multi-task Model

![](/img/note/202409282257.png)

### MOE(Mixture of Experts)

![](/img/note/202409282258.png)

### MMOE(Multi-gate Mixture-of-Experts)

![](/img/note/202409282259.png)

### ESMM(Entire Space Multi-Task Model)

![](/img/note/202409282300.png)

![](/img/note/202409282301.png)

### PLE(Progressive Layered Extraction)

![](/img/note/202409282302.png)

![](/img/note/202409282303.png)