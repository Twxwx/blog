---
title: 优化器
date: 2024-04-30 23:17:26
categories:
    - 深度学习
tags:
---

## 传统梯度更新算法

- 批量梯度下降算法（BGD）在训练的时候选用所有的训练集进行计算；随机梯度下降算法（SGD）在训练的时候只选择一个数据进行训练；小批量梯度下降算法（MBGD）在训练的时候只选择小部分数据进行训练。
![](/img/note/202406132031.png)

- 缺点
    1. 对超参数学习率比较敏感（过小导致收敛速度过慢，过大又越过极值点）。
    2. 学习率除了敏感，有时还会因其在迭代过程中保持不变，很容易造成算法被卡在鞍点的位置。
    3. 在较平坦的区域，由于梯度接近于0，优化算法会因误判，在还未到达极值点时，就提前结束迭代，陷入局部极小值。

- pytorch代码

``` python
# 因为使用的是传统的梯度下降算法，则momentum参数和nesterov参数默认即可不需要设置
torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False)
```

## 动量算法

![](/img/note/202406132032.png)

- pytorch代码

``` python
# 因为使用了动量，因此参数momentum就需要给定数值，nesterov设置为True时，将会使用NAG算法，它是动量算法的一种优化
torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False)
```

## AdaGrad算法

![](/img/note/202406132033.png)

- pytorch代码

``` python
# params – 要优化的参数。
# lr (float, optional) – 学习率 (默认: 1e-2)
# lr_decay (float, optional) – 学习率衰减 (默认: 0)
# weight_decay (float, optional) – 权重衰减 (L2 penalty) (默认: 0)
# eps (float, optional) – 为提高数字稳定性，在分母上添加了该项 (默认: 1e-10)
torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)
```

## RMSProp算法

![](/img/note/202406132034.png)

- pytorch代码

``` python
# alpha为平滑常数，momentum为动量
torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
```

## Adam算法

![](/img/note/202406132035.png)

![](/img/note/202406132036.png)

- pytorch代码

``` python
参数betas为 β1 和 β2 的集合，分别控制权重分配和之前的梯度平方的影响情况
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, *, maximize=False)
```

- Adam主要包含以下几个显著的优点：
    1. 实现简单，计算高效，对内存需求少
    2. 参数的更新不受梯度的伸缩变换影响
    3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调
    4. 更新的步长能够被限制在大致的范围内（初始学习率）
    5. 能自然地实现步长退火过程（自动调整学习率）
    6. 很适合应用于大规模的数据及参数的场景
    7. 适用于不稳定目标函数
    8. 适用于梯度稀疏或梯度存在很大噪声的问题