---
title: 损失函数
date: 2024-03-03 17:47:51
categories:
    - 深度学习
tags:
---

## MSE损失函数

![](/img/note/202409111023.png)

## KL 散度

- KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同

![](/img/note/202409111026.png)

## 交叉熵分类损失函数

- 本质上是一种对数似然函数，可用于二分类和多分类任务中

### 二分类问题

![](/img/note/202409111024.png)

```python
loss = -torch.sum(y * torch.log(probs) + (1 - y) * torch.log(1 - probs)).mean()
```

### 多分类问题

![](/img/note/202409111025.png)

```python
def cross_entropy(x: torch.Tensor, y: torch.Tensor):
    # x: (batch_size, num_classes) 经过softmax得到概率
    # y: (batch_size, num_classes) one-hot形式
    return -torch.sum(y * torch.log(x), dim=1).mean()

if __name__ == "__main__":
    input = torch.randn(4, 3)
    probs = softmax(input)
    labels = torch.tensor([0, 1, 2, 1], dtype=torch.int64)
    targets = F.one_hot(labels, num_classes = 3)
    # 不能将概率作为输入
    loss1 = F.cross_entropy(input, labels)
    # 需要将label转化为one-hot形式
    loss2 = cross_entropy(probs, targets)
```

### focal loss

- 当正负样本数量及其不平衡时，可以考虑使用FocalLoss调节正负样本的loss权重。
- 当训练样本的难易程度不平衡时，可以考虑使用FocalLoss调节难易样本的loss权重
- 预测概率距离真值越远，则样本越难

![](/img/note/202406302048.png)

![](/img/note/202406302049.png)

- alpha为超参数用于调节正负样本权重，gamma为超参数用于调节难易样本权重

## 回归损失函数

- 在目标检测中用于优化边缘框的坐标

### l1 loss

![](/img/note/202406302050.png)

### l2 loss

![](/img/note/202406302051.png)

### smooth l1 loss

![](/img/note/202406302052.png)

![](/img/note/202406302053.png)

#### 存在问题

- 在计算目标检测的 Bounding Box Loss 时，会独立地分别求出4个点的Loss，然后进行相加得到最终的Bounding Box Loss，这种做法的假设是4个点是相互独立的，实际是有一定相关性的。
- 实际评价框检测的指标是使用IoU，而IoU和Smooth L1是不等价的，多个检测框可能有相同大小的Smooth L1 Loss，但IoU可能差异很大



