---
title: 位置编码
date: 2024-01-29 18:24:52
categories:
    - 深度学习
tags:
---

## Learned Positional Embedding

- 通过可学习的Positional Embedding来编码位置信息，是预训练语言模型中最广泛的编码方式。BERT就是这种方式，后续的一系列工作Roberta，GPT2等也都是采用这种方式。

- 不具备外推的性质，长度在预设定好之后就被固定了。

### 方法

- 直接对不同的位置随机初始化一个postion embedding，然后与 word embedding 相加后输入模型。postion embedding 作为模型参数的一部分，在训练过程中进行更新。

![](/img/note/202401301128.png)


## Sinusoidal

### 方法

![](/img/note/202401301446.png)

- 通过sin和cos函数直接给出了每个位置每个维度上的绝对数值，可以认为直接初始化并固定了position embedding。

- 对比了学习式的position Embedding，与Sinusoidal的效果相近，但是Sinusoidal有更好的外推性，无需设定最大长度max_position。

### 其他特性

- 随着N的增大，周期会明显变长。
- Sinusoidal可以学习到相对位置，对于固定位置距离的 k，PE(i+k) 可以表示成 PE(i) 的线性函数。这表明Sinusoidal的编码具有对称性。同时随着 k 的增加，点积的结果会直接减少，即会存在远程衰减。

## RoPE

- RoPE旋转式位置编码是目前大模型广泛采用的一种位置编码方式。这种编码不是作用在embedding的输入层，而是作用在与Attention的计算中。
- RoPe是作用于一对位置，而非单个位置；RoPe是乘性而非加性，更符合Attention的计算方式。只要两两分组即可，可以推广到d维的场景。也可以进一步证明，随着相对位置的增加，点积后的结果会逐渐减少，呈现远端衰减。

### 方法

![](/img/note/202401301457.png)

![](/img/note/202401301459.png)

- RoPE 的 self-attention 操作的流程是，对于 token 序列中的每个 词嵌入向量，首先计算其对应的 query 和 key 向量，再对每个 token 位置都计算对应的旋转矩阵，接着对每个 token 位置的 query 和 key 向量的元素按照 两两一组 应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。


## ALiBi

- Attention with Linear Biases (ALiBi) 是一种专门提高 transformer 模型外推能力的编码，让模型在训练阶段的上下文不是很长，但是在预测阶段可以支持很长的上下文。

### 方法

- 具体而言，在attention计算之后，会添加一个固定的偏置项，这个偏置项是固定的不需学习的，只跟相对位置有关。如下所示:

![](/img/note/202401301514.png)

- 对于attention score 会根据距离添加 “惩罚”，距离越大，惩罚也越大。不同的 attention head 施加的惩罚系数 m 是不同。具体而言，假设一共有 n 个head，系数 m 是一个等比数列。

