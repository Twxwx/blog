---
title: 位置编码
date: 2024-01-29 18:24:52
categories:
    - 深度学习
tags:
---

## 为什么需要位置编码？

- CNN可以编码一定的绝对位置信息（很大程度上来自zero-padding），而RNN的序列依赖特性更是天生适合序列问题或者位置信息的建模。因此基本无须单独做位置编码。Attention 机制在不同的 token 之间并行计算，丢失了位置信息，而且难以外推。

## 不同模型使用的位置编码

![](/img/note/202403022000.png)


## Learned Positional Embedding

- 通过可学习的Positional Embedding来编码位置信息，是预训练语言模型中最广泛的编码方式。BERT就是这种方式，后续的一系列工作Roberta，GPT2等也都是采用这种方式。

- 不具备外推的性质，长度在预设定好之后就被固定了。

### 方法

- 直接对不同的位置随机初始化一个postion embedding，然后与 word embedding 相加后输入模型。postion embedding 作为模型参数的一部分，在训练过程中进行更新。

![](/img/note/202401301128.png)


## 绝对位置编码

- 绝对位置编码，顾名思义就是根据绝对位置 k 来定义位置编码，一般直接加到输入中

### Sinusoidal

#### 方法

![](/img/note/202401301446.png)

- 通过sin和cos函数直接给出了每个位置每个维度上的绝对数值，可以认为直接初始化并固定了position embedding。

- Learned Position Embedding 与 Sinusoidal 的效果相近，但是Sinusoidal有更好的外推性，无需设定最大长度max_position。

#### 其他特性

- 随着维度的增大，频率会变小。高纬度的位置编码差别不大。
- 两个绝对位置编码的点积只与相对位置 i - j 有关，所以编码具有对称性，但是是无向的。同时随着位置差的增加，点积的结果会减少，呈现出远距离衰减，也就是说两个 token 距离越远，位置编码的关系越弱。
- 经过 self-attention 之后相关性就会降低

## 相对位置编码

- 相对位置编码，顾名思义就是对相对位置 i - j 进行建模，但显然就不好像 APE 那样直接加到输入上了，既然原始 transformer 中的相对位置表达能力是在attention阶段搞丢的，那么在attention计算时引入相对位置信息。

### Ralative Position Representation

![](/img/note/202403022201.png)

- 具体做法是在计算attention score和value时各加入一个可训练的表示相对位置的参数。

### RoPE

- RoPE的工作原理是通过将每个位置的隐藏状态向量旋转一个角度来引入序列中的相对位置信息。这个角度是由位置索引和维度索引共同决定的。在数学上，RoPE通过对query和key向量进行旋转矩阵变换，使得变换后的向量带有位置信息，从而在attention矩阵上表征相对位置信息。由于这种方法是基于绝对位置编码实现的相对位置编码，因此它不需要操作Attention矩阵，有了应用到线性Attention的可能性。
- 相邻位置的编码之间有一定的相似性，而远离位置的编码之间有一定的差异性
- RoPe是乘性而非加性，直接作用于Q、K，更符合Attention的计算方式，只要两两分组即可，可以推广到d维的场景。
- 有很好的可扩展性，包括采用外扩和内插等方法
- RoPE不带有显式的远程衰减

#### 方法

![](/img/note/202401301457.png)

![](/img/note/202401301459.png)

- RoPE 的 self-attention 操作的流程是，对于 token 序列中的每个 词嵌入向量，首先计算其对应的 query 和 key 向量，再对每个 token 位置都计算对应的旋转矩阵，接着对每个 token 位置的 query 和 key 向量的元素按照 两两一组 应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。

### ALiBi

- ALiBi 引入一个线性偏置向量，用于调整注意力权重的分布。通过调整偏置向量的值，可以控制注意力权重的稀疏性和集中性，从而更好地适应不同长度的序列。这种线性偏置的引入可以帮助模型更好地处理长度外推问题，提高模型在处理长序列时的性能。

#### 方法

- 具体而言，在attention计算之后，会添加一个固定的偏置项，这个偏置项是固定的不需学习的，只跟相对位置有关。如下所示:

![](/img/note/202401301514.png)

- 对于attention score 会根据距离添加 “惩罚”，距离越大，惩罚也越大。不同的 attention head 施加的惩罚系数 m 是不同。具体而言，假设一共有 n 个head，系数 m 是一个等比数列。

