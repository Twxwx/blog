---
title: 激活函数
date: 2024-01-30 17:47:51
categories:
    - 深度学习
tags:
---

## 概述

- 神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。线性模型的表达能力通常不够，激活函数可以引入非线性因素。加入非线性激活函数后，神经网络就有可能学习到平滑的曲线来分割平面，而不是用复杂的线性组合逼近平滑曲线来分割平面，使神经网络的表示能力更强了，能够更好的拟合目标函数。

## 分类

### ReLU

![](/img/note/202403202330.png)

- 解决了gradient vanishing问题（在正区间上）
- Sigmoid和tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值，计算速度快
- 因为单侧抑制使得神经元具有了稀疏激活性，同时也会导致一些神经元永远不会被激活

### Sigmoid

![](/img/note/202403202331.png)

- 容易出现梯度消失
- 计算指数比较耗时

### Tanh

![](/img/note/202403202332.png)

### Softmax

- Softmax激活函数是一种用于多类别分类问题的激活函数，通常用于神经网络的输出层。它将输出转换为表示概率分布的数值，使得每个类别的概率值都在0和1之间，并且所有类别的概率之和等于1。这使得它适用于多类别分类问题，其中每个样本只能属于一个类别。

![](/img/note/202403210919.png)

```python
def softmax(x: torch.Tensor):
    # x: (batch_size, num_classes) 输出的logits
    exp_x = torch.exp(x - x.max(dim=-1, keepdim=True).values) # 防止指数运算溢出
    return exp_x / exp_x.sum(dim=-1, keepdim=True)

if __name__ == "__main__":
    x = torch.randn(4, 20)
    probs = softmax(x)
```

1. 将输入映射到0和1之间：Softmax函数确保每个类别的概率在0和1之间，因此可以用来表示每个类别的相对权重。
2. 归一化：Softmax函数对原始分数进行归一化，使所有类别的概率之和为1，这使得它适用于多类别互斥的分类问题。
3. 放大差异：Softmax函数会放大具有更高原始分数的类别的概率，因此可以更好地区分不同类别的可能性。

### SwiGLU

- SwiGLU激活函数的主要思想是引入一个门控机制，用于控制输入信号在激活函数中的传递方式。它由两个部分组成：GLU（Gated Linear Unit）和Swish函数。

- GLU部分接受输入信号并执行门控操作，其目的是对输入信号进行筛选和选择性放大。它由一个sigmoid激活函数作为门控器，将输入信号转换为范围在0和1之间的值。

- Swish部分是一个非线性函数，类似于ReLU（Rectified Linear Unit），它将输入信号进行非线性变换。Swish函数定义为 `x * sigmoid(x)`，其中 `sigmoid` 是S形函数。Swish函数的特点是在输入为正数时逐渐趋向于线性变换，而在输入为负数时则具有非线性的抑制效果。

![](/img/note/202403222050.png)
