---
title: 多任务学习综述
date: 2024-02-02 17:08:11
categories:
    - 深度学习
tags:
---

## 什么是多任务学习？

1. 多任务学习：
    - 给定 m 个学习任务，这 m 个任务或它们的一个子集彼此相关但不完全相同。通过使用所有 m 个任务中包含的知识，有助于改善特定模型的学习。

2. 多任务学习的特点：
    - 具有相关联任务效果相互提升作用，即同时学习多个任务，若某个任务中包含对另一个任务有用的信息，则能够提高在后者上的表现；
    - 具有正则化的效果，即模型不仅需要在一个任务上表现较好，还需要再别的任务上表现好，倾向于学习到在多个任务上表现都比较好的特征；
    - 多任务模型可以共享部分结构，降低内存占用，在推理时减少重复计算，提高推理速度。

3. MTL 处理的任务应具有一定的关联性，若同时学习两个不相关甚至冲突的任务，模型表现可能会受到损害出现经常所说的跷跷板现象，即两个任务联合学习的时候，可能一个任务效果变好，另一个任务效果变差，这个现象称为负迁移。究其本质主要是训练过程中可能出现以下 3 个问题导致的：
    - 多任务梯度方向不一致：同一组参数，不同的任务更新方向不同，导致模型参数出现震荡，任务之间出现负迁移的现象，一般出现在多个任务之间差异较大的场景。
    - 多任务收敛速度不一致：不同的任务收敛速度不一样，有的任务简单收敛速度快，有的任务困难收敛速度慢，导致模型训练一定轮数后，有的任务已经过拟合，有的任务还是欠拟合的状态；
    - 多任务 loss 取值量级差异大：不同的任务 loss 取值范围差异大，模型被 loss 比较大的任务主导，这种情况在两个任务使用不同损失函数，或者拟合值的取值差异大等情况下最为常见。


## 多任务学习的网络结构

- 一个高效的多任务网络，应同时兼顾特征共享部分和任务特定部分，既需要学习任务之间的泛化表示 以避免过拟合，又需要学习每个任务独有的特征以避免欠拟合。根据模型在处理不同任务时网络参数的共享程度，MTL 方法的网络结构可分为：

![](/img/note/202402021723.png)

### 硬参数共享 (Hard Parameter Sharing)
- 模型的主体部分共享参数，输出结构任务独立。通过在不同任务上学习共享的特征，降低模型在单个任务上过拟合的风险。

### 软参数共享 (Soft Parameter Sharing) 
- 不同任务采用独立模型，模型参数彼此约束。底层共享一部分参数，自己还有独特的一部分参数不共享；顶层有自己的参数。底层共享的、不共享的参数如何融合到一起送到顶层，是研究人员们关注的重点。


## 多任务学习中的损失函数

- 多任务学习将多个相关的任务共同训练，其总损失函数是每个任务的损失函数的加权求和式。权重的选择应能够平衡每个任务的训练，使得各任务都获得有益的提升。









