---
title: XGB模型
date: 2024-05-26 21:25:27
categories:
    - 机器学习
tags:
---

## 梯度提升决策树（GBDT）

- GBDT是一种迭代的决策树算法，该算法由多棵决策树组成，将上一颗树的预测结果与真实结果的差，即残差，作为下一棵树的预测目标，最后将所有树的结果累加起来做最终答案。GBDT中的树是回归树，GBDT用来做回归预测，调整后也可以用于分类。

![](/img/note/202407291336.png)


## 简介

- XGBoost是一个可扩展机器学习系统。严格意义上讲XGBoost并不是一种模型，而是一个可供用户轻松解决分类、回归或排序问题的软件包。它内部实现了梯度提升树(GBDT) 模型，并对模型中的算法进行了诸多优化，在取得高精度的同时又保持了极快的速度。

- - 并行处理：XGBoost 能够自动利用 CPU 的多线程进行并行处理，这使得训练过程更快，尤其是在处理大规模数据时，XGBoost 的表现优于许多其他工具包

- XGBoost在机器学习与数据挖掘领域有着极为广泛的应用，还被成功应用在工业界与学术界的各种问题中。例如商店销售额预测、高能物理事件分类、web文本分类;用户行为预测、运动检测、广告点击率预测、恶意软件分类、灾害风险预测、在线课程退学率预测。

## 优点

- 简单易用。相对其他机器学习库，用户可以轻松使用XGBoost并获得相当不错的效果。
- 高效可扩展。在处理大规模数据集时速度快效果好，对内存等硬件资源要求不高。
- 鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果。
- XGBoost内部实现提升树模型，可以自动处理缺失值。

## 缺点

- 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。
- 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习优于XGBoost。

## 原理

- 决策树构建：XGBoost 使用 CART (Classification and Regression Trees) 算法来构建决策树，它的思想是串联多个决策树模型共同进行决策。XGBoost采用迭代预测误差的方法串联。每一颗树用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果。

- 梯度提升：XGBoost通过不断地添加树来拟合预测的残差，每次添加一个树实际上是学习一个新的函数来拟合上次预测的残差。这种方法使得模型能够逐步改进预测，提高整体预测的准确性。

- 目标函数：与传统的GBDT只使用一阶导数信息不同，XGBoost对损失函数进行了二阶泰勒展开，并在目标函数中加入了正则项，这有助于更好地优化模型，减少过拟合，并提高模型的泛化能力

![](/img/note/202406071934.png)

## 训练过程

1. 特征构建
    - 对非数值进行编码。由于XGBoost无法处理字符串类型的数据，我们需要一些方法将字符串数据转化为数值。一种最简单的方法是把所有的相同类别的特征编码成同一个值，例如女=0，男=1，狗狗=2。除此之外，还有独热编码、求和编码、留一法编码等等方法可以获得更好的效果。

2. 特征重要性
    - 还可以利用 XGBoost 确定重要特征

3. 参数优化
    - 可以对参数进行调整优化

4. 损失函数
    - 回归问题可采用MSE，分类问题可采用交叉熵
