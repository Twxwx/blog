---
title: 大模型幻觉问题
date: 2024-03-05 09:59:24
categories:
    - 大模型
tags:
---

## 幻觉的定义

- 当模型生成的文本不遵循原文（Faithfulness）或者不符合事实（Factualness），我们就可以认为模型出现了幻觉的问题。

## 幻觉的类型

- 内在幻觉：生成的内容与源内容相互矛盾
- 外在幻觉：生成的内容无法从源内容中验证，既可能正确也可能错误。

``` text
源文本（天气报告）："今天北京的天气晴朗，最高气温达到了30度。"
```

``` text
内在幻觉：
生成的摘要："今天北京出现了大雨，气温下降到了20度。"
解释：这里，生成的摘要与源文本相矛盾。源文本提到天气晴朗和温度较高，而生成的摘要错误地提到了大雨和气温下降，这就是内在幻觉的一个例子。
```

``` text
外在幻觉：
生成的摘要："今天北京天气晴朗，另外，明天有可能下雨。"
解释：在这个例子中，生成的摘要正确地提到了“今天北京天气晴朗”，这与源文本一致。但是，它还添加了“明天有可能下雨”的信息，这个信息在原始报告中没有提及。虽然这个附加信息可能是正确的，但因为它无法通过源文本进行验证，因此被视为外在幻觉。
```

## 幻觉的原因

### 数据层面
- 训练数据收集过程中，众包/爬虫检索的数据可能包含虚假信息，从而让模型记忆了错误的知识；
- 过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉：

### 模型层面
- 解码算法：研究表明，如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）
- 暴露偏差：训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候。
- 参数知识：LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题。

## 解决方法

### 数据层面

#### 构建高质量数据集

- 人工标注
    - 训练数据：LLM上不可行，只适用于task-specific的幻觉问题
    - 评测数据：构建细粒度的幻觉评估benchmark用于分析幻觉的严重程度和原因

- 自动筛选
    - 利用模型筛选出可能导致幻觉的数据并剔除；
    - 预训练时给更faithful的数据加权（wiki vs. fake news），或者不使用可靠来源的数据（比如只选用经过人工审查的数据源，如wiki或者教科书，预训练）

### 模型层面

#### 模型结构

- 模型结构层面的工作往往focus在设计更能充分编码利用source information的方法，比如融入一些人类偏置，如GNN网络。
- 或者在解码时减少模型的生成随机性，因为diversity和Faithfulness往往是一个trade-off的关系，减少diversity/randomness可以变相提升Faithfulness/Factuality。
- 检索增强被证明可以显著减少幻觉问题

#### 训练方式

- 可控文本生成：将幻觉的程度作为一个可控的属性，利用可控文本生成技术进行控制。
- 多任务学习: 通过设计合适的额外任务，可以达到减轻幻觉的效果。
- 后处理：设计一个小模型专门用于fix幻觉错误。

## 未来方向

- 更细粒度的幻觉评估

- 幻觉消除：
    - 检索增强：互联网/外挂知识库(llama Index)
    - 强化学习（RLHF）
    - 知识诱导/注入
    - 直接修改LLM中错误记忆的知识：Model Editing工作，如ROME，MEMIT等


