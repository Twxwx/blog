---
title: 大模型中的 Scaling Law
date: 2024-01-28 13:49:19
categories:
    - 大模型
tags:
---

## 核心结论
![](/img/note/202401281432.png)
![](/img/note/202401281433.png)
![](/img/note/202401281434.png)

## 核心公式
![](/img/note/202401281435.png)

## LLaMA: 反Scaling Law的大模型
- 计算效率最优这个观点是针对训练阶段而言的，并不是推理阶段，实际应用中推理阶段效率更实用。

- Meta 在 LLaMA 的观点是：给定模型的目标性能，并不需要用最优的计算效率在最快时间训练好模型，而应该在更大规模的数据上，训练一个相对更小模型，这样的模型在推理阶段的成本更低，尽管训练阶段的效率不是最优的（同样的算力其实能获得更优的模型，但是模型尺寸也会更大）。根据Scaling Law，10B模型只需要200B的数据，但是作者发现7B的模型性能在1T的数据后还能继续提升。

- 所以LLaMA工作的重点是训练一系列语言模型，通过使用更多的数据，让模型在有限推理资源下有最佳的性能。

- 具体而言，确定模型尺寸后，Scaling Law给到的只是最优的数据量，或者说是一个至少的数据量，实际在训练中观察在各个指标上的性能表现，只要还在继续增长，就可以持续增加训练数据。
