---
title: 大模型量化技术
date: 2024-02-14 23:33:13
categories:
    - 大模型
tags:
---

## 量化的目的
- 是为了减少计算时间和计算能耗 。在一些场景下对能耗和时间的要求，要高于模型的指标，所以在这种情况下量化是一个必然的选择。

## 量化的定义 
- 量化一般是指将 F32 数据映射成 int8 的数据。泛指将F32映射为低 bit 的数值表示，如 int4、int8。量化的方法包括二值量化，线性量化、指数量化。

![](/img/note/202402192328.png)

1. Float32 (FP32) 。标准的 IEEE 32 位浮点表示，指数 8 位，尾数 23 位，符号 1 位，可以表示大范围的浮点数。大部分硬件都支持 FP32 运算指令。
2. Float16 (FP16) 。指数 5 位，尾数 10 位，符号 1 位。FP16 数字的数值范围远低于 FP32，存在上溢 (当用于表示非常大的数时) 和下溢 (当用于表示非常小的数时) 的风险，通过缩放损失 (loss scaling) 来缓解这个问题。
3. Bfloat16 (BF16) 。指数 8 位 (与 FP32 相同)，尾数 7 位，符号 1 位。这意味着 BF16 可以保留与 FP32 相同的动态范围。但是相对于 FP16，损失了 3 位精度。因此，在使用 BF16 精度时，大数值绝对没有问题，但是精度会比 FP16 差。
4. TensorFloat-32(TF32) 。使用 19 位表示，结合了 BF16 的范围和 FP16 的精度，是计算数据类型而不是存储数据类型。目前使用范围较小。

## INT8 量化技术
- 在训练时，为保证精度，主权重始终为 FP32。而在推理时，FP16 权重通常能提供与 FP32 相似的精度，这意味着在推理时使用 FP16 权重，仅需一半 GPU 显存就能获得相同的结果。那么是否还能进一步减少显存消耗呢？答案就是使用量化技术，最常见的就是 INT8 量化。

![](/img/note/202402192329.png)

- 对于向量中的离群值 (Emergent Features) 进行量化会导致大部分信息在处理后都丢失了。Emergent Features 的分布是有规律的。可以采用混合精度分解的量化方法：将包含了Emergent Features的几个维度从矩阵中分离出来，对其做高精度的矩阵乘法；其余部分进行量化。


## 精度提升方法 
- 部分量化，一些模型量化后精度损失往往是一些层导致的，这些层不进行量化，精度将大幅度提升
- QAT 也是精度提升的有效方法

## 量化方法
- 量化的方案多种多样，这里重点介绍对称量化和非对称量化。
- 关键性总结：
    - 对称量化能够满足整个模型量化需求（nvidia 认为，TensorRT 全部使用对称量化）
    - weight 量化中，非对称量化相比于对称量化代价较大
    - 激活函数的量化一般使用非对称量化，且量化成 uint8

![](/img/note/202402192259.png)

## 量化粒度 
- 量化的量化粒度越小， 模型的精度损失越小，但是计算量越大。
- 激活层的量化使用 per-tensor 就已经足够 。
- 卷积或者反卷积使用 per-tensor 或 per-channel
- 量化最小粒度为 per-col 或者 per-row, 实际使用中只有 per-tensor, per-channel

## 模型量化方法
- 训练后量化 (PTQ) 是将已经训练好的模型进行量化，同时只需要很少的数据或者不需要数据，少部分需要手动调整的超参数以及不需要端到端训练。这使得PTQ成为一种工程实现简单并且不需要大量计算成本的量化方法。
- 量化感知训练 (QAT) 它依赖神经网络在训练过程中进行模拟量化。虽然QAT需要进行重新训练以及调整超参数，但是在低bit时却可以比PTQ获得更接近全精度的效果。




