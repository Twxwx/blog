---
title: 大语言模型的涌现能力
date: 2024-01-17 14:37:10
categories:
    - 大模型
tags:
---

## 什么是大模型的涌现能力
- 第一类任务表现出伸缩法则：这类任务一般是知识密集型任务。随着模型规模的不断增长，任务效果也持续增长，说明这类任务对大模型中知识蕴涵的数量要求较高。
- 第二类任务表现出涌现能力：这类任务一般是由多步骤构成的复杂任务。只有当模型规模大到一定程度时，效果才会急剧增长，在模型规模小于某个临界值之前，模型基本不具备任务解决能力。
- 第三类任务随着模型规模增长，任务效果体现出一个 U 形曲线。随着模型规模增长，刚开始模型效果会呈下降趋势，但当模型规模足够大时，效果反而会提升。如果对这类任务使用 CoT 技术，这些任务的表现就会转化成伸缩法则，效果也会随着模型规模增长而持续上升。
![](/img/note/202401191958.jpeg)
![](/img/note/202401191959.jpeg)

## LLM 表现出的涌现现象
- 第一类具备涌现现象的技术是 In Context Learning，用户给出几个例子，大模型不需要调整模型参数，就能够处理好任务。利用 In Context Learning，已经发现在各种类型的下游任务中，大语言模型都出现了涌现现象，体现在在模型规模不够大的时候，各种任务都处理不好，但是当跨过某个模型大小临界值的时候，大模型就突然能比较好地处理这些任务。
![](/img/note/202401201502.jpeg)

- 第二类具备涌现现象的技术是思维链( CoT )。CoT 本质上是一种特殊的 few shot prompt，就是说对于某个复杂的比如推理问题，用户把一步一步的推导过程写出来，并提供给大语言模型，这样大语言模型就能做一些相对复杂的推理任务。 
![](/img/note/202401201507.jpeg)

## LLM 模型规模和涌现能力的关系
- 要想出现涌现能力，模型规模大小和具体任务有一定的绑定关系。
- 训练模型的时候，可以考虑先增加训练数据，降低模型参数量，把模型做小，先把模型参数利用充分，在这个基础上，再继续增加数据，并推大模型规模。

## 模型训练中的顿悟现象
![](/img/note/202401201523.jpeg)
顿悟现象。如上图所示，对于一个训练数据较少的数学任务（通常是数字求和取余数的问题），研究人员发现一种新奇的现象。比如我们将数据集切成两块，50% 数据作为训练集（图中红线展示了随着训练过程往后走，任务指标的变化情况），50% 的数据作为验证集（图中绿线的走势展示了训练动态）。在学习数字求和取余这个任务时，它的训练动态会经历三个阶段：
- 记忆期：红线对应的训练数据指标突然走高，代表模型记住了50%的训练数据的结果，而绿线对应的验证集指标接近0，说明模型完全没有泛化能力，就是说没有学会这个任务的规律。所以这个阶段模型只是在单纯地记忆训练数据。
- 平台期：这个阶段是记忆期的延续，体现为验证集合效果仍然很差，说明模型仍然没有学会规律。
- 泛化期：这个阶段验证集合效果突然变好，这说明突然之间，模型学会了任务里的规律，也就是我们说的，出现了顿悟现象，突然就学明白了。
Grokking 本质上是在学习输入数字的一个好的表征。由初始化向记忆期再到顿悟现象出现的过程，模型逐步开始学习任务的任务结构。

### 顿悟和涌现能力的区别
Grokking描述的是模型训练动态中的表现，而涌现表达的是模型规模变化时的任务表现，虽然走势相近，但两者不是一回事。

## LLM 涌现能力的可能原因
- 猜想一：任务的评价指标不够平滑
![](/img/note/202401201727.jpeg)
- 猜想二：复杂任务 vs 子任务
![](/img/note/202401201732.jpeg)

