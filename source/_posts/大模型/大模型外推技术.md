---
title: 大模型外推技术
date: 2024-03-02 21:00:27
categories:
    - 大模型
tags:
---

## 什么是大模型外推性

- 外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了 512 个 token 的文本，那么在预测时如果输入超过 512 个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

- 长度外推应当不以牺牲远程依赖为代价

## 解决方法

### RoPE 位置编码

![](/img/note/202403022300.png)

- 通过旋转矩阵来实现位置编码的外推，即可以通过旋转矩阵来生成超过预训练长度的位置编码。这样可以提高模型的泛化能力和鲁棒性。

### 窗口截断

![](/img/note/202403022301.png)

- Sliding Window Mask：由于强行截断了窗口外的注意力，所以这个方案并不满足“不牺牲远程依赖的能力”的原则。
- Λ-shape Window Mask：开头的几个Token很重要，不能扔掉。

### 位置内插

- 将预测的长文本的位置编码乘上缩放因子，缩放到训练长度范围内。
- 位置内插避免了远处的位置越界问题，但这同时压缩了邻近Token的距离，严重扰乱了模型的局部分辨率，而众所周知语言模型本身就是一个非常依赖于局部关系的任务，所以扰乱了局部自然就没法预测准了。

### 保近压远

![](/img/note/202403022302.png)

- Leaky ReRoPE通过一个非常直接的思路实现了这一点：它先设定一个窗口大小 w 内，将相对位置分为两部分，在窗口内不改变相对位置实现“局部不失真”，在窗口外使用位置内插实现“远处不越界”

