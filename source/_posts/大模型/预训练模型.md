---
title: 预训练模型
date: 2024-02-04 16:54:51
categories:
    - 大模型
tags:
---

## 概述

- 在 Transformer 作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫 Transformer 从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在 Transformer 结构中，以供下游任务使用。

## 预训练的发动机 模型结构

### Encoder-AE 结构

![](/img/note/202402041706.png)

- 除了 Encoder-Decoder 结构外，貌似对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。

### Decoder-AR 结构（Casual LM）

![](/img/note/202402041707.png)

- 除了 Encoder-Decoder 结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。

### Encoder-Decoder 结构

![](/img/note/202402041708.png)

- Encoder 阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词，还能看到Encoder侧的所有输入单词

### Prefix LM

![](/img/note/202402041709.png)

- Prefix LM其实是Encoder-Decoder模型的变体，相当于Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割方式是通过在Transformer内部使用Attention Mask来实现的。

## 多模态预训练

- 本质上，多模态预训练要学习的知识是两种模态之间，或者多种模态之间的知识单元映射关系。比如对于文字-图片这两种多模态信息来说，我们可以把图片想像成一种特殊类型的语言，多模态预训练希望让模型学会这两种不同模态之间的语义映射关系。






