---
title: 大模型存在问题及解决方法
date: 2024-10-09 23:33:49
categories:
    - 大模型
tags:
---

## 过度消耗数据和算力
- 大模型的参数量已达到万亿级别，训练数据规模和算力消耗与参数规模成正比。

## 灾难性遗忘

### 问题
- 在新任务上训练会损害之前任务的性能，导致模型失去在预训练阶段学到的通用知识。因为只用领域数据非常容易过拟合

### 解决方法
1. 多任务学习：在训练过程中同时考虑多个任务，包括领域特定的任务和通用任务，这有助于模型在学习新知识的同时保留旧知识。
2. 正则化技术：应用正则化方法，如L1或L2正则化，限制模型参数在训练过程中的变化，以减少对原有知识的遗忘。
3. 知识蒸馏：使用知识蒸馏技术，将预训练模型的知识传递给领域特定模型，即使领域模型专注于新任务，也能保持一定的通用能力。
4. 记忆重放：在训练过程中周期性地重放一些通用数据，以提醒模型不要忘记其原有的知识。

## 黑盒模型逻辑推理能力弱
- 大模型缺乏“分而治之”能力，在处理需要逻辑、数值推理等复杂问题时表现不佳，无法举一反三、触类旁通。

## 幻觉问题
- 大模型不知道自己错了，也不知道为啥错，更做不到知错就改。

## 无法获取实时信息

- 由于 LLM 的训练成本非常高，它无法实时更新其知识库。因此，LLM 可能无法提供最新的信息或跟上快速变化的情况。
