---
title: the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)
date: 2024-03-29 11:27:48
categories:
    - 大模型
tags:
---

## 重要结论

- 在训练语料中包含代码可以提高模型处理自然语言任务的准确率。
- 侧重训练一个规模和 GPT-3 一样的多语言语言模型
1. 使用了ALiBi Positional Embeddings 位置嵌入
2. 在嵌入层后面加了一层 LayerNorm 层，好处是训练时提升了稳定性；坏处是会影响到推断是零样本学习的泛化能力
3. Tokenizer和GPT-2不同，自己训练了一个；
4. 训练语料不同，包含多语言和编程语言；
5. 预训练任务不同，除了无条件生成任务外，在指令数据集上进行了多任务微调；
6. 模型训练不同，采用Megatron-DeepSpeed并行技术对模型训练进行加速；

## 背景

- 以往的大规模语言模型，例如GPT，GPT-2，GPT-3主要是基于英语语料训练的。 本文旨在基于一个包含46种自然语言和13种编程语言的语料库，训练一个规模和GPT-3相当的多语言语言模型。为此，本文首先在多语言数据集上进行模型预训练，然后在指令数据集上微调模型。

## 实现细节

### 数据集

![](/img/note/202403301342.png)

- 预训练数据处理过程如图所示。从网上获取的源数据中包含大量的非自然语言，例如预处理错误、SEO页面或垃圾邮件。第一步是去除这些非自然语言。为此，本文定义了一组质量指标来过滤非自然语言；第二步移除重复的文档，并删除数据中包含的个人身份信息。最后，在prompt数据集上进行多任务微调。

### 模型结构

![](/img/note/202403301343.png)
