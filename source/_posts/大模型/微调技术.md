---
title: 微调技术
date: 2024-02-23 21:45:39
categories:
    - 大模型
tags:
---

## Prefix Tuning

- 人工设计的 prompt 中那些对应自然语言的真实 token 要经过嵌入层，被向量化得到的 token 是离散的，得到的结果大概率是次优的。相较而言，连续化的 prefix prompt 搜索更具优势。使用虚拟的 token 不对应任何自然语言中的 subword，它们仅仅是一些连续的向量

![](/img/note/202403062112.png)

## Prompt Tuning

- 离散的 prompts（指人工设计 prompts 提示语加入到模型）方法，成本比较高，并且效果不太好。

![](/img/note/202403062111.png)

## P-Tuning

![](/img/note/202403062113.png)

## P-Tuning v2

![](/img/note/202403062114.png)

## Lora（Low-Rank Adaptation）

![](/img/note/202403062115.png)

## QLora

![](/img/note/202403062116.png)

- QLoRA 同时结合了模型量化 Quant 和 LoRA 参数微调两种方法

- QLoRA 针对模型权重(weight)做量化，采用的是对称量化算法
    1. 4位NormalFloat量化：这是一种改进量化的方法。它确保每个量化仓中有相同数量的值，即采用新的 NF (NormalFloat)数据类型，它是对于正态分布权重而言信息理论上最优的数据类型，同时，NF 类型有助于缓解异常值的影响
    2. 双量化：QLoRa的作者将其定义如下：对量化常量再次量化以节省额外内存的过程。即Double Quant，对于量化后的 scale 数据做进一步的量化
    3. QLoRa还有统一内存分页：它依赖于NVIDIA统一内存管理，自动处理CPU和GPU之间的页到页传输，它可以保证GPU处理无错，特别是在GPU可能耗尽内存的情况下