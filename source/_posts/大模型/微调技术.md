---
title: 微调技术
date: 2024-02-23 21:45:39
categories:
    - 大模型
tags:
---

## Prefix Tuning

- 人工设计的 prompt 中那些对应自然语言的真实 token 要经过嵌入层，被向量化得到的 token 是离散的，得到的结果大概率是次优的。相较而言，连续化的 prefix prompt 搜索更具优势。使用虚拟的 token 不对应任何自然语言中的 subword，它们仅仅是一些连续的向量

![](/img/note/202403062112.png)

## Prompt Tuning

- 离散的 prompts（指人工设计 prompts 提示语加入到模型）方法，成本比较高，并且效果不太好。

![](/img/note/202403062111.png)

## P-Tuning

![](/img/note/202403062113.png)

## P-Tuning v2

![](/img/note/202403062114.png)

## Lora（Low-Rank Adaptation）

![](/img/note/202403062115.png)

## QLora

- 其实就是量化版的 Lora

![](/img/note/202403062116.png)



