---
title: 大模型微调
date: 2024-02-23 21:45:39
categories:
    - 大模型
tags:
---

## Prefix Tuning

- 人工设计的 prompt 中那些对应自然语言的真实 token 要经过嵌入层，被向量化得到的 token 是离散的，得到的结果大概率是次优的。相较而言，连续化的 prefix prompt 搜索更具优势。使用虚拟的 token 不对应任何自然语言中的 subword，它们仅仅是一些连续的向量

![](/img/note/202403062112.png)

## Prompt Tuning

- 离散的 prompts（指人工设计 prompts 提示语加入到模型）方法，成本比较高，并且效果不太好。

![](/img/note/202403062111.png)

## P-Tuning

![](/img/note/202403062113.png)

## P-Tuning v2

![](/img/note/202403062114.png)

## LoRA（Low-Rank Adaptation）

### 简单介绍一下Lora，以及lora为什么有效？

- LoRA，即LLMs的低秩适应，是参数高效微调最常用的方法。研究表明，深度学习的矩阵往往是过参数化的。LoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。

- Transformer的权重矩阵包括Attention模块里用于计算query, key, value的Wq，Wk，Wv以及多头attention的Wo,以及MLP层的权重矩阵，LoRA只应用于Attention模块中的4种权重矩阵，而且通过消融实验发现同时调整 Wq 和 Wv 会产生最佳结果。

![](/img/note/202410092321.png)

- **缺点**：LoRA 需要预先指定每个增量矩阵的本征秩 r 相同，忽略了在微调预训练模型时，权重矩阵的重要性在不同模块和层之间存在显著差异，并且只训练了Attention，没有训练FFN，事实上FFN更重要。

## AdaLora

1. 不能预先指定矩阵的秩，需要动态更新增量矩阵的r，因为权重矩阵的重要性在不同模块和层之间存在显著差异。
2. 需要找到更加重要的矩阵，分配更多的参数，裁剪不重要的矩阵。找到重要的矩阵，可以提升模型效果；而裁剪不重要的矩阵，可以降低参数计算量，降低模型效果差的风险。

- 为了解决Lora的缺点提出了AdaLoRA，它根据权重矩阵的重要性得分，在权重矩阵之间自适应地分配参数预算。

![](/img/note/202410092322.png)

## QLora

![](/img/note/202410092323.png)

- QLoRA 同时结合了模型量化 Quant 和 LoRA 参数微调两种方法，进一步减少内存占用，可以微调更大参数的模型。由于量化操作，比Lora稍慢。

- QLoRA 使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLORA 有一种低精度存储数据类型（4 bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用 QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16 位矩阵乘法。
    1. bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。
    2. 双量化：对量化常量再次量化以节省额外内存的过程。对于量化后的 scale 数据做进一步的量化，减少存储空间。
    3. 分页优化器：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。




