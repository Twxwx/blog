---
title: 大模型和RAG
date: 2024-02-01 17:47:51
categories:
    - 大模型
tags:
---

## 概述

- 检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。当我们将大模型应用于实际业务场景时会发现，通用的基础大模型基本无法满足我们的实际业务需求，主要有以下几方面原因：

    - 知识的局限性：模型自身的知识完全源于它的训练数据，而现有的主流大模型（ChatGPT、文心一言、通义千问…）的训练集基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备。
    - 幻觉问题：所有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种幻觉问题的区分是比较困难的，因为它要求使用者自身具备相应领域的知识。
    - 数据安全性：对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身的私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍。

- RAG（中文为检索增强生成） = 检索技术 + LLM 提示。例如，我们向 LLM 提问一个问题（answer），RAG 从各种数据源检索相关的信息，并将检索到的信息和问题（answer）注入到 LLM 提示中，LLM 最后给出答案。

## 实现过程

![](/img/note/202403162111.png)

### 分块和向量化

- 为文档内容创建向量索引，然后在运行时搜索与查询向量余弦距离最近的向量索引，这样就可以找到与查询内容最接近语义的文档。

#### 分块

- Transformer 模型具有固定的输入序列长度，即使输入上下文窗口很大，一个句子或几个句子的向量也比几页文本的向量更能代表其语义含义，因此对数据进行分块（将初始文档拆分为一定大小的块，而不会失去其含义）。

- 块的大小是一个需要重点考虑的问题。块的大小取决于所使用的嵌入模型以及模型需要使用 token 的容量。如基于 BERT 的句子转换器，最多需要 512 个 token；OpenAI ada-002 能够处理更长的序列，如 8191 个 token，保证 LLM 有足够的上下文来推理。

#### 向量化

- 选择一个搜索优化的模型来嵌入 Embedding。有很多选项，比如 bge-large 或 E5 嵌入系列。

### 搜索索引

![](/img/note/202403162112.png)

### 重排和过滤

- 在使用上述任一算法获取检索结果后，通过过滤、重新排名或一些转换来优化这些结果。可以基于相似度评分、关键词、元数据进行过滤。

### 查询转换

- 查询转换是一系列技术，使用 LLM 作为推理引擎来修改用户输入以提高检索质量。有很多技术实现可供选择。

![](/img/note/202403162113.png)

1. 对于复杂的查询，大语言模型能够将其拆分为多个子查询。这些子查询会并行执行，检索到的信息随后被汇总到一个 LLM 提示词中。

2. Step-back prompting 使用 LLM 生成一个更通用的查询，以此检索到更通用或高层次的上下文，用于为我们的原始查询提供答案。同时执行原始查询的检索，并在最终答案生成步骤中将两个上下文发送到 LLM。

3. 查询重写使用 LLM 来重新表述初始查询，以改进检索。

### 聊天引擎

- 检索到的上下文与原始用户消息一起传递给 LLM 作为对话上下文以生成答案。

![](/img/note/202403162114.png)

### 查询路由

- 查询路由是 LLM 驱动的决策步骤，决定在给定用户查询的情况下下一步该做什么——选项通常是总结、对某些数据索引执行搜索或尝试许多不同的路由，然后将它们的输出综合到一个答案中。

- 查询路由器还用于选择数据存储位置来处理用户查询。这些数据存储位置可能是多样的，比如传统的向量存储、图形数据库或关系型数据库，或者是不同层级的索引系统。在处理多文档存储时，通常会用到摘要索引和文档块向量索引这两种不同的索引。

### 智能体

![](/img/note/202403162115.png)

### 响应合成

- 这是任何 RAG 管道的最后一步：根据检索的所有上下文和初始用户查询生成答案。

- 最简单的方法是将所有获取的上下文（高于某个相关性阈值）与查询一起连接并提供给 LLM。但是，与往常一样，还有其他更复杂的选项，涉及多个 LLM 调用，以优化检索到的上下文并生成更好的答案。

- 响应合成的主要方法有：

    - 通过将检索到的上下文逐块发送到 LLM 来优化答案
    - 概括检索到的上下文，以适应提示
    - 根据不同的上下文块生成多个答案，然后将它们连接或概括起来。



