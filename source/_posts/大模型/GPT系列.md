---
title: GPT系列（Generative Pre-trained Transformer）
date: 2024-01-20 17:47:51
categories:
    - 大模型
tags:
---

## GPT-1

[论文链接](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

1. 核心思路：
- 在大量无标记数据集上训练 Transformer 的 Decoders 来做 NLG （语言生成），得到优秀的生成模型。然后根据下游任务微调（fine-tune）模型。

2. 背景：
- 有大量未标记的文本语料库，但用于学习特定任务的标记数据却很少。
- 模型难以泛化，根据一种任务训练的模型难以用在其他任务上。

3. 训练方法：
- 首先在大的无标记语料 A 上训练语言模型 LM_A 。
- 在 LM_A 上增加少量神经网络层来完成特定任务例如语言生成等， 采用有标记语料B来有监督地训练 LM_A，这个过程中 LM_A 的参数不固定，可学习。

4. 相较于 BERT：
- BERT 采用 Encoders 做完形填空，GPT 采用 Decoders 做预测未来。
- GPT 训练的难度更大，导致前期效果没有 BERT 好。但 GPT 相较于 BERT 更适合做生成类的任务如翻译、摘要。（ GPT 技术路线更难，天花板很可能也更高）

5. 结构

![](/img/note/202401261447.png)

6. 训练流程
- 无监督预训练
使用大量无标记的数据。根据给定的前 i-1 个 token，预测第 i 个 token。训练使用的是基于最大似然估计的损失函数，即让模型预测的概率分布尽可能接近实际下一个单词的分布。

![](/img/note/202401271757.png)

- 有监督微调
使用有标记的数据。使用 L_1 目标函数公式 pre-train 一个模型后，就可以用这个模型的参数去进行有监督的 fine-tune 的任务了。fine-tune 时，GPT 的训练目标是优化在任务数据上的效果，必然会将 pre-train 时学习到的参数覆盖或忘记，降低了模型的通用性。通过结合 pre-train 和 fine-tune 的目标函数，可以在训练模型解决下游任务的同时，保留一定模型通过 pre-train 学习到的参数，即模型的通用性。

![](/img/note/202401271800.png)


## GPT-2

[论文链接](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

1. 背景：
- 当前的语言模型对数据的微弱变动十分敏感，像一个"狭隘"的专家系统。OperAI 想要构建能执行很多任务的通用系统，不需要为每个任务单独训练。

2. 核心：
- GPT-2 弃用 fine-tune 采用 zero-shot。

3. 相较于 GPT-1：
- 去掉有监督的 fine-tune，仅剩无监督的 pre-train，不用再为特定任务 fine-tune。
- 增加高质量数据集
- 增加网路参数，参数量达到 15 亿，同时期 BERT 只有 3 亿。词汇表数量增加到 50257 个，最大的上下文大小从GPT的 512 提升到了1024，batchsize增加到512。
- 调整并增加 Trm，将 LN 放到每个 sub-layer 之前，最后一个 self-attention block 之后加一个LN。

4. 训练策略
- prompt 是为下游任务设计的一种模板或者范式。prompt 能够帮助预训练模型"回忆"起自己预训练学到的知识，实质是将下游任务和预训练任务的统一（近似）。采用 prompt 时，模型不利用样本做训练，即不对预训练模型的参数做任何更新。
- Fine-tuning：用预训练模型去"迁就"下游任务，即根据下游任务添加辅助 loss 然后反向梯度更新预训练模型中的参数，这样的话也许不能很好的激发预训练模型的潜能，降低了模型的通用性。
- Prompting：用下游任务去"迁就"预训练模型，即尽量让下游任务和预训练相似，充分发挥预训练模型的潜能，大幅提高模型通用性，训练模型难度也加大。
- GPT-2 所用的策略接近 prompt，即给预训练语言模型的一个线索、提示，帮助它可以更好的理解人类的问题，能很好的发挥其语言生成的优势。


## GPT-3

[论文链接](https://arxiv.org/pdf/2005.14165.pdf)

1. 核心
- GPT-2 验证了在大规模高质量数据情况下，可以用 zero-shot learning 替换 fine-tuning，让模型在预训练时自己学习各种 prompt 对应的回答。
- 对于所有下游任务，GPT-3 不利用样本做训练，即不做模型参数的任何更新。
- GPT-3 更进一步，加大模型参数和训练数据集，并且提出了一些新策略。

2. In-context Learning（情景学习）
- pre-train + fine-tune 就好比我们（模型）自学了一系列知识，但解题前老师（有标记样本数据）会教我们做几道相似的样题（微调模型），而 zero-shot learning 好比去掉了老师讲解样题，就要求我们（模型）在训练中自己学习到各种题目的解法，所以我们必须训练的更多。训练完成后我们（模型）也会更强大，更通用。
- Zero-shot Learning (零样本学习)： 在没有任何样本/示例情况下，让预训练语言模型完成特定任务。放弃 fine-tune，仅通过大规模多领域的数据 pre-train，让模型在 Zero-shot setting 自己学会解决多任务的问题。GPT-2 证明了这是可行的策略。
- One shot Learning (单样本学习)： 指在只有一个样本/示例的情况下，预训练语言模型完成特定任务。
- Few-shot Learning (少样本或小样本学习)：指在只有少量样本/示例的情况下，预训练语言模型完成特定任务。

3. IFT（Instruction Fine-Tuning） & CoT（Chain-of-thought） 
- IFT （指令微调）的数据通常是由人工手写指令和语言模型引导的指令实例的集合。 这些指令数据由三个主要组成部分组成：指令、输入和输出，对于给定的指令，可以有多个输入和输出实例。

![](/img/note/202401271924.png)

- CoT（思维链）简言之就是给模型推理步骤的prompt，让其学习人类的思考/推理方式，从而让模型具备基本的推理能力，最终可以求解一些简单甚至相对复杂的数学推理能力。

![](/img/note/202401271925.png)

