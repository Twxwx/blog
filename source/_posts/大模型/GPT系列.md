---
title: GPT系列（Generative Pre-trained Transformer）
date: 2024-01-20 17:47:51
categories:
    - 大模型
tags:
---

## 参数量对比

![](/img/note/202403050930.png)

## 模型对比

![](/img/note/202403050931.png)

## GPT-1

[论文链接](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

1. 核心思路：
- 在大量无标记数据集上训练 Transformer 的 Decoders 来做 NLG （语言生成），得到优秀的生成模型。然后根据下游任务微调（fine-tune）模型。

2. 背景：
- 有大量未标记的文本语料库，但用于学习特定任务的标记数据却很少。
- 模型难以泛化，根据一种任务训练的模型难以用在其他任务上。

3. 训练方法：
- 首先在大的无标记语料 A 上训练语言模型 LM_A 。
- 在 LM_A 上增加少量神经网络层来完成特定任务例如语言生成等， 采用有标记语料B来有监督地训练 LM_A，这个过程中 LM_A 的参数不固定，可学习。

4. 相较于 BERT：
- BERT 采用 Encoders 做完形填空，GPT 采用 Decoders 做预测未来。
- GPT 训练的难度更大，导致前期效果没有 BERT 好。但 GPT 相较于 BERT 更适合做生成类的任务如翻译、摘要。（ GPT 技术路线更难，天花板很可能也更高）

5. 结构

![](/img/note/202401261447.png)

6. 训练流程
- 无监督预训练
使用大量无标记的数据。根据给定的前 i-1 个 token，预测第 i 个 token。训练使用的是基于最大似然估计的损失函数，即让模型预测的概率分布尽可能接近实际下一个单词的分布。

![](/img/note/202401271757.png)

- 有监督微调
使用有标记的数据。使用 L_1 目标函数公式 pre-train 一个模型后，就可以用这个模型的参数去进行有监督的 fine-tune 的任务了。fine-tune 时，GPT 的训练目标是优化在任务数据上的效果，必然会将 pre-train 时学习到的参数覆盖或忘记，降低了模型的通用性。通过结合 pre-train 和 fine-tune 的目标函数，可以在训练模型解决下游任务的同时，保留一定模型通过 pre-train 学习到的参数，即模型的通用性。

![](/img/note/202401271800.png)


## GPT-2

[论文链接](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

1. 背景：
- 当前的语言模型对数据的微弱变动十分敏感，像一个"狭隘"的专家系统。OperAI 想要构建能执行很多任务的通用系统，不需要为每个任务单独训练。

2. 核心：
- GPT-2 弃用 fine-tune 采用 zero-shot。

3. 相较于 GPT-1：
- 去掉有监督的 fine-tune，仅剩无监督的 pre-train，不用再为特定任务 fine-tune。
- 增加高质量数据集
- 增加网路参数，参数量达到 15 亿，同时期 BERT 只有 3 亿。词汇表数量增加到 50257 个，最大的上下文大小从GPT的 512 提升到了1024，batchsize增加到512。
- 调整并增加 Trm，将 LN 放到每个 sub-layer 之前，最后一个 self-attention block 之后加一个LN。

4. 训练策略
- prompt 是为下游任务设计的一种模板或者范式。prompt 能够帮助预训练模型"回忆"起自己预训练学到的知识，实质是将下游任务和预训练任务的统一（近似）。采用 prompt 时，模型不利用样本做训练，即不对预训练模型的参数做任何更新。
- Fine-tuning：用预训练模型去"迁就"下游任务，即根据下游任务添加辅助 loss 然后反向梯度更新预训练模型中的参数，这样的话也许不能很好的激发预训练模型的潜能，降低了模型的通用性。
- Prompting：用下游任务去"迁就"预训练模型，即尽量让下游任务和预训练相似，充分发挥预训练模型的潜能，大幅提高模型通用性，训练模型难度也加大。
- GPT-2 所用的策略接近 prompt，即给预训练语言模型的一个线索、提示，帮助它可以更好的理解人类的问题，能很好的发挥其语言生成的优势。


## GPT-3

[论文链接](https://arxiv.org/pdf/2005.14165.pdf)

1. 核心
- GPT-2 验证了在大规模高质量数据情况下，可以用 zero-shot learning 替换 fine-tuning，让模型在预训练时自己学习各种 prompt 对应的回答。
- 对于所有下游任务，GPT-3 不利用样本做训练，即不做模型参数的任何更新。
- GPT-3 更进一步，加大模型参数和训练数据集，并且提出了一些新策略。

### 采用交替密度和局部带状稀疏注意力模式

![](/img/note/202403050932.png)

2. In-context Learning（情景学习）
- pre-train + fine-tune 就好比我们（模型）自学了一系列知识，但解题前老师（有标记样本数据）会教我们做几道相似的样题（微调模型），而 zero-shot learning 好比去掉了老师讲解样题，就要求我们（模型）在训练中自己学习到各种题目的解法，所以我们必须训练的更多。训练完成后我们（模型）也会更强大，更通用。
- Zero-shot Learning (零样本学习)： 在没有任何样本/示例情况下，让预训练语言模型完成特定任务。放弃 fine-tune，仅通过大规模多领域的数据 pre-train，让模型在 Zero-shot setting 自己学会解决多任务的问题。GPT-2 证明了这是可行的策略。
- One shot Learning (单样本学习)： 指在只有一个样本/示例的情况下，预训练语言模型完成特定任务。
- Few-shot Learning (少样本或小样本学习)：指在只有少量样本/示例的情况下，预训练语言模型完成特定任务。

3. IFT（Instruction Fine-Tuning） & CoT（Chain-of-thought） 
- IFT （指令微调）的数据通常是由人工手写指令和语言模型引导的指令实例的集合。 这些指令数据由三个主要组成部分组成：指令、输入和输出，对于给定的指令，可以有多个输入和输出实例。

![](/img/note/202401271924.png)

- CoT（思维链）简言之就是给模型推理步骤的prompt，让其学习人类的思考/推理方式，从而让模型具备基本的推理能力，最终可以求解一些简单甚至相对复杂的数学推理能力。

![](/img/note/202401271925.png)

## InstructGPT && ChatGPT

[论文链接](https://arxiv.org/pdf/2203.02155.pdf)

### 区别
- ChatGPT和InstructGPT的训练方式相同，不同点仅仅是它们采集数据上有所不同。考虑到ChatGPT仅仅被用在对话领域，猜测ChatGPT在数据采集上有两个不同：1. 提高了对话类任务的占比；2. 将提示的方式转换Q&A的方式。

### 指示学习（Instruct Learning）和提示学习（Prompt Learning）
- 指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。不同的是Prompt是激发语言模型的补全能力，例如根据上半句生成下半句，或是完形填空等。Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。我们可以通过下面的例子来理解这两个不同的学习方式：

    - 提示学习：给女朋友买了这个项链，她很喜欢，这个项链太____了。
    - 指示学习：判断这句话的情感：给女朋友买了这个项链，她很喜欢。选项：A=好；B=一般；C=差。

- 指示学习的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示学习都是针对一个任务的。泛化能力不如指示学习。我们可以通过下图来理解微调，提示学习和指示学习。

![](/img/note/202401281622.png)

### 训练流程

![](/img/note/202401281706.png)

#### 根据采集的SFT数据集对GPT-3进行有监督的微调（Supervised FineTune，SFT）

1. SFT训练集
- 因为 GPT-3 是一个基于提示学习的生成模型，因此SFT数据集也是由提示-答复对组成的样本。在这个数据集中，标注工的工作是根据内容自己编写指示，并且要求编写的指示满足下面三点：
    - 简单任务：labeler给出任意一个简单的任务，同时要确保任务的多样性；
    - Few-shot任务：labeler给出一个指示，以及该指示的多个查询-相应对；
    - 用户相关的：从接口中获取用例，然后让labeler根据这些用例编写指示。

2. 训练过程
- 和GPT-3一致，作者发现让模型适当过拟合有助于后面两步的训练


#### 收集人工标注的对比数据，训练奖励模型（Reword Model，RM）

1. RM数据集
- 用来训练第2步的奖励模型，我们也需要为InstructGPT/ChatGPT的训练设置一个奖励目标。这个奖励目标不必可导，但是一定要尽可能全面且真实的对齐我们需要模型生成的内容。很自然的，我们可以通过人工标注的方式来提供这个奖励，通过人工对可以给那些涉及偏见的生成内容更低的分从而鼓励模型不去生成这些人类不喜欢的内容。InstructGPT/ChatGPT的做法是先让模型生成一批候选文本，然后通过labeler根据生成数据的质量对这些生成内容进行排序。

2. 训练过程
- 将去掉最后一个非嵌入层的SFT模型作为基座模型，训练一个模型，输入提示和响应，输出标量奖励。
![](/img/note/202401281727.png)


#### 使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型

1. PPO数据集
- PPO数据没有进行标注，它均来自GPT-3的API的用户。既又不同用户提供的不同种类的生成任务，其中占比最高的包括生成任务（45.6%），QA（12.4%），头脑风暴（11.2%），对话（8.4%）等。

2. 训练过程
![](/img/note/202401281737.jpg)
![](/img/note/202401281732.png)

### RLHF的局限性
1. RLHF范式训练出来的模型虽然效果更好，但是仍然会输出有害或不准确的文本。
2. 人工标注的成本是非常高昂的。
3. RLHF的流程还可以继续改进，改进RL优化器尤为重要。PPO是一种相对较旧的RL算法。